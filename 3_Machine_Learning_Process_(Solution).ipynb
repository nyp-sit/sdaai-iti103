{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti103/blob/master/3_Machine_Learning_Process_(Solution).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlcN8ctTgVbP"
      },
      "source": [
        "# California Housing Dataset\n",
        "## Getting the data\n",
        "We will be using the California housing Prices dataset.  This dataset was based on data from the 1990 California census.  \n",
        "\n",
        "https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzNmjWJsib_P"
      },
      "source": [
        "## Basic Data Analysis\n",
        "Q1  Let's load the data from the url and perform basic data analysis.\n",
        "\n",
        "*   Check the Sample size (Hint: shape)\n",
        "*   Check the features (Hint: describe method)\n",
        "*   Check for missing values using info method\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWU_gij3yrfJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "url = 'https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv'\n",
        "df = pd.read_csv(url)\n",
        "print(df.head())\n",
        "print('>>> Check the sample size:')\n",
        "# use shape\n",
        "\n",
        "print('>>> Check for the features: ')\n",
        "# use describe\n",
        "\n",
        "print('>>> Check for missing values')\n",
        "# use info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8iwY8Jxg9Rj"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import pandas as pd\n",
        "url = 'https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv'\n",
        "df = pd.read_csv(url)\n",
        "print(df.head())\n",
        "print('>>> Check the sample size:')\n",
        "print(df.shape)\n",
        "print('>>> Check for the features: ')\n",
        "print(df.describe())\n",
        "print('>>> Check for missing values')\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cor4QuVt01wu"
      },
      "source": [
        "Q2. Creates a histogram and inspect the attributes distribution to look for insights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1Xy59zyyqcP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Creates a histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywG7VZISkuG7"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import matplotlib.pyplot as plt\n",
        "df.hist(bins=50, figsize=(20,15))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Test Set\n",
        "## Simple Random Sampling"
      ],
      "metadata": {
        "id": "ILWn9BwNOMFp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty3mxijKytVh"
      },
      "source": [
        "Q3.  Split the training set to 80/20 using random sampling technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLxWrsW2IoaG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# random sampling, 80/20 split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DUvCrLUzIA-"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "def split_train_test(data, test_ratio):\n",
        "  shuffled_indices = np.random.permutation(len(data))\n",
        "  test_set_size = int(len(data) * test_ratio)\n",
        "  test_indices = shuffled_indices[:test_set_size]\n",
        "  train_indices = shuffled_indices[test_set_size:]\n",
        "  return data.iloc[train_indices], data.iloc[test_indices]\n",
        "\n",
        "train_set, test_set = split_train_test(df, 0.2)\n",
        "print(len(train_set), 'train', len(test_set), 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LcggVHYU3YB"
      },
      "source": [
        "## Using Stratified Sampling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGTP6MCwJYxi"
      },
      "source": [
        "Stratified random sampling is a method of sampling that involves the division of a population into smaller sub-groups known as strata. In stratified random sampling, the strata are formed based on members' shared attributes or characteristics such as income or educational attainment.  The following source shows you how you could use Stratified Sampling to split the data into training and testing set. We use the pd.cut() to bin the median income into 5 categories (e.g. 0 to 1.5 is cat 1, 1.5 to 3.0 is cat 2, 3.0 to 4.5 is cat 3, 4.5 to 6 is cat 4, and 6 to infinity is cat 5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VPh_Rs2VAOP"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "\n",
        "df[\"income_cat\"] = pd.cut(df[\"median_income\"],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])\n",
        "df.income_cat.value_counts().sort_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ0c3Q3W_bD5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "strat_train_set, strat_test_set = train_test_split(df, shuffle=True, stratify=df['income_cat'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYTKQXwA_bD5"
      },
      "outputs": [],
      "source": [
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "#df['income_cat'].value_counts() / len(df)\n",
        "for train_index, test_index in split.split(df, df['income_cat']):\n",
        "  strat_train_set = df.loc[train_index]\n",
        "  strat_test_set = df.loc[test_index]\n",
        "\n",
        "print(len(strat_train_set), 'train', len(strat_test_set), 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B2imo9FZouN"
      },
      "source": [
        "Measure the income category proportions in the test set generated with random sampling and stratified sampling.  The test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is quite skewed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAmYzNZcrV-z"
      },
      "outputs": [],
      "source": [
        "def income_cat_props(data):\n",
        "  return data['income_cat'].value_counts()/len(data)\n",
        "\n",
        "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "compare_props = pd.DataFrame({\n",
        "    'Overall': income_cat_props(df),\n",
        "    'Stratified': income_cat_props(strat_test_set),\n",
        "    'Random': income_cat_props(test_set)\n",
        "}).sort_index()\n",
        "\n",
        "compare_props['Rand. %error'] = 100 * compare_props['Random'] / compare_props['Overall'] - 100\n",
        "compare_props['Strat. %error'] = 100 * compare_props['Stratified'] / compare_props['Overall'] - 100\n",
        "\n",
        "compare_props"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AazWTvU7UxLw"
      },
      "source": [
        "## Discover and Visualize the Data to Gain Insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv6HI_TL0gjW"
      },
      "source": [
        "Q4. We shall discover and visualize the Data to gain more insights.   Let's create a copy of the housing data so that we can experiment with it without affecting the training set.  Use the copy method to create a new copy of the stratified training data set and creates a scatter plot of all the districts to visualize poppulation density with respect to the longitude and latitude data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86ea5QL6XpnE"
      },
      "outputs": [],
      "source": [
        "# scatter plot of all districts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a5kwNDuvcp3"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "housing = strat_train_set.copy()\n",
        "housing.plot(kind='scatter',  x='longitude', y='latitude', alpha=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5ALI2ccPFFL"
      },
      "source": [
        "In this scatter plot, we examine the housing prices with the radius of each circle representing the district's poulation (option s) and the color represents the price (option c).  The predefined color map (option cmap) called jet, ranges from blue for low values to red for high values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNsB3-KRX-YA"
      },
      "outputs": [],
      "source": [
        "# scatter plot with color map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-HA5Zd40cJt"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import matplotlib.pyplot as plt\n",
        "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4,\n",
        "             s=housing['population'] / 100, label='population',\n",
        "             c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slBdVYQRY8CZ"
      },
      "source": [
        "### Looking for Correlations\n",
        "\n",
        "Q5.  Compute the standard correlation coefficient between every pair of attributes using the corr() method, and examine how much each attribute with the median house value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92ZPpJGJYR-T"
      },
      "outputs": [],
      "source": [
        "# compute the standard correlation coefficient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBECn0Eb2Mpn"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "corr_matrix = housing.corr()\n",
        "corr_matrix['median_house_value'].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDmlOm_ZZhoN"
      },
      "source": [
        "Another way to check for correlation between attributes is to use scatter_matrix function which plots every numerical attribute against every other numerical attribute.  The most promising attribute the predict the median house value seems to be the median income.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFZGa130YnJd"
      },
      "outputs": [],
      "source": [
        "# scatter matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMcC1Yhd261q"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']\n",
        "scatter_matrix(housing[attributes], figsize=(12,8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjWe62VAb6vG"
      },
      "source": [
        "Q6.  Creates the scatter plot to show the  correlation of medina_income and median_housing_value, ane examine the visualisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL6ivLnQY5v8"
      },
      "outputs": [],
      "source": [
        "# scatter plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtqUFuB_4n8Q"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O17nh_b_bD9"
      },
      "outputs": [],
      "source": [
        "housing[ (housing['median_house_value'] < 360000) & (housing['median_house_value'] > 340000) ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Lw6jHtJduUL"
      },
      "source": [
        "### Experimenting with Attribute Combinations\n",
        "Q7.  Try out various attribute combinations before actually preparing the data for Machine Learning.  Creates these 3 new attributes:\n",
        "\n",
        "\n",
        "*   rooms per household ( total rooms / households )\n",
        "*   bedrooms per room (total bedrooms / total rooms)\n",
        "*   population per house hold (population / households)\n",
        "\n",
        "And compute the correlation matrix again to examine the correlation between median house values and all the other attributes, including these 3 new attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jBryg58ZClh"
      },
      "outputs": [],
      "source": [
        "# Creates new attributes and compute the correlation matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtW3XHnP7u0c"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "housing['rooms_per_household'] = housing['total_rooms']/housing['households']\n",
        "housing['bedrooms_per_room'] = housing['total_bedrooms']/housing['total_rooms']\n",
        "housing['population_per_household'] = housing['population']/housing['households']\n",
        "corr_matrix = housing.corr()\n",
        "corr_matrix['median_house_value'].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnTMPaRg_9Vh"
      },
      "source": [
        "## Data Cleaning\n",
        "Q8 There are some missing values within the data set for total_bedroom attribute, fix it by\n",
        "\n",
        "\n",
        "*   Get rid of the corresponding districts\n",
        "*   Get rid of the whole attribute\n",
        "*   Set the values to some value, such as zero, mean or median\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqhKVK1RZTa-"
      },
      "outputs": [],
      "source": [
        "# Data cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dsdxhQUAF5F"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "housing = strat_train_set.drop('median_house_value', axis=1)\n",
        "housing_labels=strat_train_set['median_house_value'].copy()\n",
        "# Get rid of districts with missing values\n",
        "housing.dropna(subset=['total_bedrooms'])\n",
        "# Get rid of the attribute\n",
        "housing.drop('total_bedrooms', axis = 1)\n",
        "median = housing['total_bedrooms'].median()\n",
        "# Set the missing values to median \n",
        "housing['total_bedrooms'].fillna(median)\n",
        "print(housing.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gk09UzdaruI"
      },
      "outputs": [],
      "source": [
        "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
        "sample_incomplete_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAaPNnuPxKdu"
      },
      "source": [
        "Scikit Learn provides a handy class to take care of missing values: Imputer. \n",
        "* Creates an Imputer instance, specifying that you want to replace the missing attribute's missing values with the median of that attribute.   \n",
        "* Creates a copy of the data without the text attribute ocean_proximity\n",
        "*  Fit the imputer instance to the training data using the fit() method:\n",
        "*  Display the statistics_ instance variable of the imputer object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoUnWjzEWDGA"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "housing_num = housing.drop('ocean_proximity', axis = 1)\n",
        "imputer.fit(housing_num)\n",
        "imputer.statistics_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpgsSEEQYVs3"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "X = imputer.transform(housing_num)\n",
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index = list(housing.index.values))\n",
        "housing_tr.loc[sample_incomplete_rows.index.values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55tKSTaYYWnq"
      },
      "outputs": [],
      "source": [
        "housing_cat = housing[['ocean_proximity']]\n",
        "housing_cat.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Text and Categorical Attributes"
      ],
      "metadata": {
        "id": "LbqXK2nDPQRi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGoNrCDqZl8g"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
        "housing_cat_encoded[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnULAWsBaXx4"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
        "housing_cat_encoded[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YvhDaJlbRmM"
      },
      "outputs": [],
      "source": [
        "encoder.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xha0axhObrq2"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
        "housing_cat_1hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxclZdbddKjq"
      },
      "outputs": [],
      "source": [
        "housing_cat_1hot.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o0-MCdn5OvB"
      },
      "source": [
        "The categorical attribute ocean_proximity was left out because it is a text attribute so we cannot compute its median.  Transform the text categories to integer categories, then from integer categories to one-hot vectors in one shot using the LabelBinarizer class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFcSpFlEjPHP"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "encoder = LabelBinarizer()\n",
        "housing_cat_1hot = encoder.fit_transform(housing_cat)\n",
        "housing_cat_1hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6mMMTvN8oie"
      },
      "source": [
        "Creates custom transformers to create taks such as combining specific attributes, such as rooms per household, population per household and bedrooms per room.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KtmZ-qmpXmL"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# column index\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
        "\n",
        "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
        "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
        "    def fit(self, X, y=None):\n",
        "        return self  # nothing else to do\n",
        "    def transform(self, X, y=None):\n",
        "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
        "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
        "        if self.add_bedrooms_per_room:\n",
        "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
        "        else:\n",
        "            return np.c_[X, rooms_per_household, population_per_household]\n",
        "\n",
        "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
        "housing_extra_attribs = attr_adder.transform(housing.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zNZMIxWputS"
      },
      "outputs": [],
      "source": [
        "housing_extra_attribs = pd.DataFrame(\n",
        "    housing_extra_attribs,\n",
        "    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"])\n",
        "housing_extra_attribs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automate with Pipelines"
      ],
      "metadata": {
        "id": "7r7_M0BzPglS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOR74bdOD2cH"
      },
      "source": [
        "Scikit-Learn provides the Pipeline class to help with data transformation steps that need to be executed in the right order.  A full pipeline handling both numerical and categorical attributes may look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhMzHsLFkZZ5"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('attribs_adder', CombinedAttributesAdder()),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "housing_num_tr = num_pipeline.fit_transform(housing_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjfGlEZskzYU"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "    ])\n",
        "\n",
        "housing_prepared = full_pipeline.fit_transform(housing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgYXLq-yk-fl"
      },
      "outputs": [],
      "source": [
        "housing_prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9AoRtELlHGJ"
      },
      "outputs": [],
      "source": [
        "housing_prepared.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qz0qSxvFa1G"
      },
      "source": [
        "## Select and Train a Model\n",
        "## Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSE5BDyrFeLl"
      },
      "source": [
        "Q9. Train a Linear Regression model with the prepared data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zflGnMOLbsMm"
      },
      "outputs": [],
      "source": [
        "# Linear Regression model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r6rrl73lafo"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBSW0oTfGKHk"
      },
      "source": [
        "Try it out on a few instances from the training set.  It works, but the predictions are not exactly accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXKuMBsJT4Ey"
      },
      "outputs": [],
      "source": [
        "some_data = housing.iloc[:5]\n",
        "some_labels = housing_labels.iloc[:5]\n",
        "some_data_prepared = full_pipeline.transform(some_data)\n",
        "\n",
        "print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
        "print(\"Labels:\", list(some_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arqord_TUuOj"
      },
      "outputs": [],
      "source": [
        "some_data_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Root Mean Square error (RMSE)"
      ],
      "metadata": {
        "id": "fN1RVyJkP9nB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8zv8V11Gell"
      },
      "source": [
        "Q10.  Measure the regression model RMSE on the whole training set using Scikit-Learn's mean_squared_error functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2xYGQh9qKer"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkQT3b-rUzVD"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "ms8mpc-XQPEn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIYvYwpTGwfM"
      },
      "source": [
        "Q11.  Measure the regression model MAE on the whole training set using Scikit-Learn's mean_absolute_error functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5a0l1LvU3-b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTze-yjQ7-hE"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mae = mean_absolute_error(housing_labels, housing_predictions)\n",
        "lin_mae"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "Y5uavHr6QRoP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szF92KA2HTyz"
      },
      "source": [
        "Let's try to use a DecisionTreeRegressor to train the model for comparison purpose.  The model seemed to have badly overfit the data.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIw2dHHoU9kd"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "tree_reg.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bolmVLY4VCsy"
      },
      "outputs": [],
      "source": [
        "\n",
        "housing_predictions = tree_reg.predict(housing_prepared)\n",
        "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "tree_rmse = np.sqrt(tree_mse)\n",
        "tree_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wpd7ygJHIxc"
      },
      "source": [
        "## Evaluation using Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn4LzAS1HMNE"
      },
      "source": [
        "Q12.  Use the cross validation feature to splits the training set into 10 distinct folds, then trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time.  Display the scores using the display_scores function given above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFSnNa6DWfRU"
      },
      "outputs": [],
      "source": [
        "def display_scores(scores):\n",
        "    print(\"Scores:\", scores)\n",
        "    print(\"Mean:\", scores.mean())\n",
        "    print(\"Standard deviation:\", scores.std())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIDm6U-5vI2w"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jngqjKSgW5BZ"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
        "print(scores)\n",
        "tree_rmse_scores = np.sqrt(-scores)\n",
        "print(tree_rmse_scores)\n",
        "display_scores(tree_rmse_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUY3COkXJWCo"
      },
      "source": [
        "Q13. Compute the same scores for the LInear Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "0Oe4HnDVvP9V"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_T_ocsvXA2h"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
        "print(lin_scores)\n",
        "lin_rmse_scores = np.sqrt(-lin_scores)\n",
        "display_scores(lin_rmse_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aziQXl2JJ6oF"
      },
      "source": [
        "## Fine Tune your model\n",
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-gk857J_jL"
      },
      "source": [
        "One way to fine tune the model is to use Scikit-Learn's GridSearch CV to evaluate all the possible combiniations of hyperparameter values that you want it to experiment with. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pb4Vpmo2bFIG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "param_grid = [\n",
        "    # try 12 (3×4) combinations of hyperparameters\n",
        "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
        "    # then try 6 (2×3) combinations with bootstrap set as False\n",
        "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "  ]\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "grid_search.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31fEXNO3bINI"
      },
      "outputs": [],
      "source": [
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XElKJxFgbPgJ"
      },
      "outputs": [],
      "source": [
        "grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6llsxYmbTyN"
      },
      "outputs": [],
      "source": [
        "cvres = grid_search.cv_results_\n",
        "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "    print(np.sqrt(-mean_score), params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glIMkIplboty"
      },
      "outputs": [],
      "source": [
        "\n",
        "pd.DataFrame(grid_search.cv_results_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-xQ3hGA2Eug"
      },
      "outputs": [],
      "source": [
        "final_model = grid_search.best_estimator_\n",
        "\n",
        "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
        "y_test = strat_test_set[\"median_house_value\"].copy()\n",
        "\n",
        "X_test_prepared = full_pipeline.transform(X_test)\n",
        "final_predictions = final_model.predict(X_test_prepared)\n",
        "\n",
        "final_mse = mean_squared_error(y_test, final_predictions)\n",
        "final_rmse = np.sqrt(final_mse)\n",
        "final_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEhQAYXQNcM5"
      },
      "source": [
        "## Evaluate System on the Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0YBImjeNhuJ"
      },
      "source": [
        "Q14  Evaluate the final model on the test set.  Use the Linear Regression model for the final testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hPyYHOkwITQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8T7TFrY9eRM"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "final_model = lin_reg\n",
        "\n",
        "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
        "y_test = strat_test_set[\"median_house_value\"].copy()\n",
        "\n",
        "X_test_prepared = full_pipeline.transform(X_test)\n",
        "final_predictions = final_model.predict(X_test_prepared)\n",
        "\n",
        "final_mse = mean_squared_error(y_test, final_predictions)\n",
        "final_rmse = np.sqrt(final_mse)\n",
        "final_rmse"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "3_Machine_Learning_Process_(Solution).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}