{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Process 2021S1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti103/blob/master/session-3/Machine_Learning_Process_2021S1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlcN8ctTgVbP"
      },
      "source": [
        "# Practical: Quick Run through ML using Scikit-Learn\n",
        "\n",
        "In this lab, you will be using the Califironia housing Prices dataset to predict the housing price in California.\n",
        "\n",
        "At the end of the session, you will learn how to:\n",
        "\n",
        "\n",
        "1.   Perform exploratary data analysis\n",
        "2.   Perform data preparation\n",
        "3.   Train and validate model\n",
        "4.   Fine Tune Model\n",
        "5.   Evaluate System on Test Set\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUd852Bs29m6"
      },
      "source": [
        "## Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5T_7DFP2yxx"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNk6srtW3swN"
      },
      "source": [
        "## Getting the data\n",
        "\n",
        "We will be using the California housing Prices dataset.  This dataset was based on data from the 1990 California census.  \n",
        "\n",
        "https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeQwwacq2nLf"
      },
      "source": [
        "import pandas as pd\n",
        "url = 'https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv'\n",
        "\n",
        "# we can directly fetch the data and convert the csv into a pandas dataframe using the following:\n",
        "df = pd.read_csv(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzNmjWJsib_P"
      },
      "source": [
        "## Exploratary Data Analysis\n",
        "\n",
        "As in all Machine Learning project, understanding your data and doing some exploratory data analysis is one of the essential tasks.\n",
        "\n",
        "**Exercise 1**\n",
        "\n",
        "Answer the following by completing the code cell below: \n",
        "\n",
        "*   What many records (samples) do we have? (Hint: shape)\n",
        "*   What are the different features we have?  (Hint: describe() method)\n",
        "*   Are there any missing values? (Hint: info() method)\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "```\n",
        "print('>>> Check the sample size:')\n",
        "print(df.shape)\n",
        "print('>>> Check for the features: ')\n",
        "print(df.describe())\n",
        "print('>>> Check for missing values')\n",
        "print(df.info())\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWU_gij3yrfJ"
      },
      "source": [
        "print('>>> Check the sample size:')\n",
        "# use shape\n",
        "print(df.shape)\n",
        "print('>>> Check for the features: ')\n",
        "# use describe\n",
        "print(df.describe())\n",
        "print('>>> Check for missing values')\n",
        "# use info\n",
        "print('>>> Check the sample size:')\n",
        "print(df.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYnOhQpIAORC"
      },
      "source": [
        "Before we proceed with more data exploration, it is often a good practice for us to first set aside a part of data as dataset as test set, so as to prevent us from snooping information/pattern from the test set and 'overfit' ourselves (and eventually our model) to the test set.\n",
        "\n",
        "We can either random shuffle the data and split them into train/test split using scikit learn's train_test_split() method, e.g. \n",
        "\n",
        "```\n",
        "train_set, test_set = train_test_split(df, 0.2)\n",
        "print(len(train_set), 'train', len(test_set), 'test')\n",
        "```\n",
        "\n",
        "This is\n",
        "generally fine if your dataset is large enough (especially relative to the\n",
        "number of attributes), but if it is not, you run the risk of introducing a\n",
        "significant sampling bias.  Your train set may not have a representative distribution as your eventual test set or real-world data. \n",
        "\n",
        "If based on the domain experts inputs, who feel that income distribution is a key for good prediction, we want to make sure our train and test set has the same income distribution. So we may want to split in such a way that train/test set has same distribution of income categories, e.g. This can be done by stratified sampling.\n",
        "\n",
        "Before that let's take a closer look at the income distribution using histogram\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPomfmUbD03_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "ca6f4be7-1911-4502-dfae-f19ee56ad3d1"
      },
      "source": [
        "plt.hist(df.median_income)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2247., 7436., 6098., 2990., 1060.,  428.,  178.,   93.,   47.,\n",
              "          63.]),\n",
              " array([ 0.4999 ,  1.94992,  3.39994,  4.84996,  6.29998,  7.75   ,\n",
              "         9.20002, 10.65004, 12.10006, 13.55008, 15.0001 ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9xQoAieEXXU"
      },
      "source": [
        "most median\n",
        "income values are clustered around 1.5 to 6 (i.e., $15,000–$60,000), but some\n",
        "median incomes go far beyond 6. It is important to have a sufficient number\n",
        "of instances in your dataset for each stratum, or else the estimate of a\n",
        "stratum’s importance may be biased. This means that you should not have too\n",
        "many strata, and each stratum should be large enough. \n",
        "\n",
        "We can use the pd.cut() to bin the median income into 5 categories (e.g. 0 to 1.5 is cat 1, 1.5 to 3.0 is cat 2, 3.0 to 4.5 is cat 3, 4.5 to 6 is cat 4, and 6 to infinity is cat 5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LcggVHYU3YB"
      },
      "source": [
        "### Using Stratified Sampling \n",
        "\n",
        "Stratified random sampling is a method of sampling that involves the division of a population into smaller sub-groups known as strata. In stratified random sampling, the strata are formed based on members' shared attributes or characteristics such as income or educational attainment.  The following source shows you how you could use Stratified Sampling to split the data into training and testing set. We use the pd.cut() to bin the median income into 5 categories (e.g. 0 to 1.5 is cat 1, 1.5 to 3.0 is cat 2, 3.0 to 4.5 is cat 3, 4.5 to 6 is cat 4, and 6 to infinity is cat 5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VPh_Rs2VAOP"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "\n",
        "df[\"income_cat\"] = pd.cut(df[\"median_income\"],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])\n",
        "df.income_cat.value_counts().sort_index()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u48aZBp-FIN8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DHJqBHLFKA_"
      },
      "source": [
        "**Exercise** \n",
        "\n",
        "Now split the data again but using stratified smapling based on the new feature column ['income_cat']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teg9milD2Qdc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "strat_train_set, strat_test_set = train_test_split(df, shuffle=True, stratify=df['income_cat'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B2imo9FZouN"
      },
      "source": [
        "Measure the income category proportions in the test set generated with random sampling and stratified sampling.  The test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is quite skewed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAmYzNZcrV-z"
      },
      "source": [
        "def income_cat_props(data):\n",
        "  return data['income_cat'].value_counts()/len(data)\n",
        "\n",
        "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "compare_props = pd.DataFrame({\n",
        "    'Overall': income_cat_props(df),\n",
        "    'Stratified': income_cat_props(strat_test_set),\n",
        "    'Random': income_cat_props(test_set)\n",
        "}).sort_index()\n",
        "\n",
        "compare_props['Rand. %error'] = 100 * compare_props['Random'] / compare_props['Overall'] - 100\n",
        "compare_props['Strat. %error'] = 100 * compare_props['Stratified'] / compare_props['Overall'] - 100\n",
        "\n",
        "compare_props"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cor4QuVt01wu"
      },
      "source": [
        "Q2. Creates a histogram and inspect the attributes distribution to look for insights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1Xy59zyyqcP"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Creates a histogram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ywG7VZISkuG7"
      },
      "source": [
        "#@title\n",
        "import matplotlib.pyplot as plt\n",
        "df.hist(bins=50, figsize=(20,15))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLxWrsW2IoaG"
      },
      "source": [
        "import numpy as np\n",
        "# random sampling, 80/20 split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "-DUvCrLUzIA-"
      },
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "def split_train_test(data, test_ratio):\n",
        "  shuffled_indices = np.random.permutation(len(data))\n",
        "  test_set_size = int(len(data) * test_ratio)\n",
        "  test_indices = shuffled_indices[:test_set_size]\n",
        "  train_indices = shuffled_indices[test_set_size:]\n",
        "  return data.iloc[train_indices], data.iloc[test_indices]\n",
        "\n",
        "train_set, test_set = split_train_test(df, 0.2)\n",
        "print(len(train_set), 'train', len(test_set), 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AazWTvU7UxLw"
      },
      "source": [
        "## Discover and Visualize the Data to Gain Insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv6HI_TL0gjW"
      },
      "source": [
        "Q4. We shall discover and visualize the Data to gain more insights.   Let's create a copy of the housing data so that we can experiment with it without affecting the training set.  Use the copy method to create a new copy of the stratified training data set and creates a scatter plot of all the districts to visualize poppulation density with respect to the longitude and latitude data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86ea5QL6XpnE"
      },
      "source": [
        "# scatter plot of all districts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "9a5kwNDuvcp3"
      },
      "source": [
        "#@title\n",
        "housing = strat_train_set.copy()\n",
        "housing.plot(kind='scatter',  x='longitude', y='latitude', alpha=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5ALI2ccPFFL"
      },
      "source": [
        "In this scatter plot, we examine the housing prices with the radius of each circle representing the district's poulation (option s) and the color represents the price (option c).  The predefined color map (option cmap) called jet, ranges from blue for low values to red for high values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNsB3-KRX-YA"
      },
      "source": [
        "# scatter plot with color map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "w-HA5Zd40cJt"
      },
      "source": [
        "#@title\n",
        "import matplotlib.pyplot as plt\n",
        "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4,\n",
        "             s=housing['population'] / 100, label='population',\n",
        "             c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slBdVYQRY8CZ"
      },
      "source": [
        "### Looking for Correlations\n",
        "\n",
        "Q5.  Compute the standard correlation coefficient between every pair of attributes using the corr() method, and examine how much each attribute with the median house value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92ZPpJGJYR-T"
      },
      "source": [
        "# compute the standard correlation coefficient\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "bBECn0Eb2Mpn"
      },
      "source": [
        "#@title\n",
        "corr_matrix = housing.corr()\n",
        "corr_matrix['median_house_value'].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDmlOm_ZZhoN"
      },
      "source": [
        "Another way to check for correlation between attributes is to use scatter_matrix function which plots every numerical attribute against every other numerical attribute.  The most promising attribute the predict the median house value seems to be the median income.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFZGa130YnJd"
      },
      "source": [
        "# scatter matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "aMcC1Yhd261q"
      },
      "source": [
        "#@title\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']\n",
        "scatter_matrix(housing[attributes], figsize=(12,8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjWe62VAb6vG"
      },
      "source": [
        "Q6.  Creates the scatter plot to show the  correlation of medina_income and median_housing_value, ane examine the visualisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL6ivLnQY5v8"
      },
      "source": [
        "# scatter plot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "dtqUFuB_4n8Q"
      },
      "source": [
        "#@title\n",
        "housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fe84feS2Qdh"
      },
      "source": [
        "housing[ (housing['median_house_value'] < 360000) & (housing['median_house_value'] > 340000) ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Lw6jHtJduUL"
      },
      "source": [
        "### Experimenting with Attribute Combinations\n",
        "Q7.  Try out various attribute combinations before actually preparing the data for Machine Learning.  Creates these 3 new attributes:\n",
        "\n",
        "\n",
        "*   rooms per household ( total rooms / households )\n",
        "*   bedrooms per room (total bedrooms / total rooms)\n",
        "*   population per house hold (population / households)\n",
        "\n",
        "And compute the correlation matrix again to examine the correlation between median house values and all the other attributes, including these 3 new attributes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jBryg58ZClh"
      },
      "source": [
        "# Creates new attributes and compute the correlation matrix\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "WtW3XHnP7u0c"
      },
      "source": [
        "#@title\n",
        "housing['rooms_per_household'] = housing['total_rooms']/housing['households']\n",
        "housing['bedrooms_per_room'] = housing['total_bedrooms']/housing['total_rooms']\n",
        "housing['population_per_household'] = housing['population']/housing['households']\n",
        "corr_matrix = housing.corr()\n",
        "corr_matrix['median_house_value'].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnTMPaRg_9Vh"
      },
      "source": [
        "## Data Cleaning\n",
        "Q8 There are some missing values within the data set for total_bedroom attribute, fix it by\n",
        "\n",
        "\n",
        "*   Get rid of the corresponding districts\n",
        "*   Get rid of the whole attribute\n",
        "*   Set the values to some value, such as zero, mean or median\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqhKVK1RZTa-"
      },
      "source": [
        "# Data cleaning\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "1dsdxhQUAF5F"
      },
      "source": [
        "#@title\n",
        "housing = strat_train_set.drop('median_house_value', axis=1)\n",
        "housing_labels=strat_train_set['median_house_value'].copy()\n",
        "# Get rid of districts with missing values\n",
        "housing.dropna(subset=['total_bedrooms'])\n",
        "# Get rid of the attribute\n",
        "housing.drop('total_bedrooms', axis = 1)\n",
        "median = housing['total_bedrooms'].median()\n",
        "# Set the missing values to median \n",
        "housing['total_bedrooms'].fillna(median)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gk09UzdaruI"
      },
      "source": [
        "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
        "sample_incomplete_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAaPNnuPxKdu"
      },
      "source": [
        "Scikit Learn provides a handy class to take care of missing values: Imputer. \n",
        "* Creates an Imputer instance, specifying that you want to replace the missing attribute's missing values with the median of that attribute.   \n",
        "* Creates a copy of the data without the text attribute ocean_proximity\n",
        "*  Fit the imputer instance to the training data using the fit() method:\n",
        "*  Display the statistics_ instance variable of the imputer object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoUnWjzEWDGA"
      },
      "source": [
        "#@title\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "housing_num = housing.drop('ocean_proximity', axis = 1)\n",
        "imputer.fit(housing_num)\n",
        "imputer.statistics_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpgsSEEQYVs3"
      },
      "source": [
        "#@title\n",
        "X = imputer.transform(housing_num)\n",
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index = list(housing.index.values))\n",
        "housing_tr.loc[sample_incomplete_rows.index.values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55tKSTaYYWnq"
      },
      "source": [
        "housing_cat = housing[['ocean_proximity']]\n",
        "housing_cat.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGoNrCDqZl8g"
      },
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
        "housing_cat_encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnULAWsBaXx4"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
        "housing_cat_encoded[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YvhDaJlbRmM"
      },
      "source": [
        "encoder.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xha0axhObrq2"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
        "housing_cat_1hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxclZdbddKjq"
      },
      "source": [
        "housing_cat_1hot.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o0-MCdn5OvB"
      },
      "source": [
        "The categorical attribute ocean_proximity was left out because it is a text attribute so we cannot compute its median.  Transform the text categories to integer categories, then from integer categories to one-hot vectors in one shot using the LabelBinarizer class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFcSpFlEjPHP"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "encoder = LabelBinarizer()\n",
        "housing_cat_1hot = encoder.fit_transform(housing_cat)\n",
        "housing_cat_1hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6mMMTvN8oie"
      },
      "source": [
        "Creates custom transformers to create taks such as combining specific attributes, such as rooms per household, population per household and bedrooms per room.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KtmZ-qmpXmL"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# column index\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
        "\n",
        "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
        "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
        "    def fit(self, X, y=None):\n",
        "        return self  # nothing else to do\n",
        "    def transform(self, X, y=None):\n",
        "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
        "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
        "        if self.add_bedrooms_per_room:\n",
        "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
        "        else:\n",
        "            return np.c_[X, rooms_per_household, population_per_household]\n",
        "\n",
        "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
        "housing_extra_attribs = attr_adder.transform(housing.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zNZMIxWputS"
      },
      "source": [
        "housing_extra_attribs = pd.DataFrame(\n",
        "    housing_extra_attribs,\n",
        "    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"])\n",
        "housing_extra_attribs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOR74bdOD2cH"
      },
      "source": [
        "Scikit-Learn provides the Pipeline class to help with data transformation steps that need to be executed in the right order.  A full pipeline handling both numerical and categorical attributes may look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhMzHsLFkZZ5"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('attribs_adder', CombinedAttributesAdder()),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "housing_num_tr = num_pipeline.fit_transform(housing_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjfGlEZskzYU"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "    ])\n",
        "\n",
        "housing_prepared = full_pipeline.fit_transform(housing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgYXLq-yk-fl"
      },
      "source": [
        "housing_prepared"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9AoRtELlHGJ"
      },
      "source": [
        "housing_prepared.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qz0qSxvFa1G"
      },
      "source": [
        "## Select and Train a Mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSE5BDyrFeLl"
      },
      "source": [
        "Q9. Train a Linear Regression model with the prepared data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zflGnMOLbsMm"
      },
      "source": [
        "# Linear Regression model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "7r6rrl73lafo"
      },
      "source": [
        "#@title\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing_prepared, housing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBSW0oTfGKHk"
      },
      "source": [
        "Try it out on a few instances from the training set.  It works, but the predictions are not exactly accurate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXKuMBsJT4Ey"
      },
      "source": [
        "some_data = housing.iloc[:5]\n",
        "some_labels = housing_labels.iloc[:5]\n",
        "some_data_prepared = full_pipeline.transform(some_data)\n",
        "\n",
        "print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
        "print(\"Labels:\", list(some_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arqord_TUuOj"
      },
      "source": [
        "some_data_prepared"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8zv8V11Gell"
      },
      "source": [
        "Q10.  Measure the regression model RMSE on the whole training set using Scikit-Learn's mean_squared_error functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2xYGQh9qKer"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "XkQT3b-rUzVD"
      },
      "source": [
        "#@title\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIYvYwpTGwfM"
      },
      "source": [
        "Q11.  Measure the regression model MAE on the whole training set using Scikit-Learn's mean_absolute_error functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5a0l1LvU3-b"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "zTze-yjQ7-hE"
      },
      "source": [
        "#@title\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mae = mean_absolute_error(housing_labels, housing_predictions)\n",
        "lin_mae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szF92KA2HTyz"
      },
      "source": [
        "Let's try to use a DecisionTreeRegressor to train the model for comparison purpose.  The model seemed to have badly overfit the data.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIw2dHHoU9kd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7236a3d2-d321-45fd-c33b-830684174de9"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "tree_reg.fit(housing_prepared, housing_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
              "                      max_features=None, max_leaf_nodes=None,\n",
              "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                      min_samples_leaf=1, min_samples_split=2,\n",
              "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                      random_state=42, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bolmVLY4VCsy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3894488-f1e8-4bd6-fda4-c3952bdf458f"
      },
      "source": [
        "\n",
        "housing_predictions = tree_reg.predict(housing_prepared)\n",
        "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "tree_rmse = np.sqrt(tree_mse)\n",
        "tree_rmse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wpd7ygJHIxc"
      },
      "source": [
        "## Evaluation using Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn4LzAS1HMNE"
      },
      "source": [
        "Q12.  Use the cross validation feature to splits the training set into 10 distinct folds, then trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time.  Display the scores using the display_scores function given above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFSnNa6DWfRU"
      },
      "source": [
        "def display_scores(scores):\n",
        "    print(\"Scores:\", scores)\n",
        "    print(\"Mean:\", scores.mean())\n",
        "    print(\"Standard deviation:\", scores.std())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIDm6U-5vI2w"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jngqjKSgW5BZ"
      },
      "source": [
        "#@title\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
        "tree_rmse_scores = np.sqrt(-scores)\n",
        "display_scores(tree_rmse_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUY3COkXJWCo"
      },
      "source": [
        "Q13. Compute the same scores for the LInear Regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "0Oe4HnDVvP9V"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "d_T_ocsvXA2h"
      },
      "source": [
        "#@title\n",
        "\n",
        "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
        "lin_rmse_scores = np.sqrt(-lin_scores)\n",
        "display_scores(lin_rmse_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aziQXl2JJ6oF"
      },
      "source": [
        "## Model Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-gk857J_jL"
      },
      "source": [
        "One way to fine tune the model is to use Scikit-Learn's GridSearch CV to evaluate all the possible combiniations of hyperparameter values that you want it to experiment with. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb4Vpmo2bFIG"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "param_grid = [\n",
        "    # try 12 (3×4) combinations of hyperparameters\n",
        "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
        "    # then try 6 (2×3) combinations with bootstrap set as False\n",
        "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "  ]\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "grid_search.fit(housing_prepared, housing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31fEXNO3bINI"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XElKJxFgbPgJ"
      },
      "source": [
        "grid_search.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6llsxYmbTyN"
      },
      "source": [
        "cvres = grid_search.cv_results_\n",
        "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "    print(np.sqrt(-mean_score), params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glIMkIplboty"
      },
      "source": [
        "\n",
        "pd.DataFrame(grid_search.cv_results_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-xQ3hGA2Eug"
      },
      "source": [
        "final_model = grid_search.best_estimator_\n",
        "\n",
        "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
        "y_test = strat_test_set[\"median_house_value\"].copy()\n",
        "\n",
        "X_test_prepared = full_pipeline.transform(X_test)\n",
        "final_predictions = final_model.predict(X_test_prepared)\n",
        "\n",
        "final_mse = mean_squared_error(y_test, final_predictions)\n",
        "final_rmse = np.sqrt(final_mse)\n",
        "final_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEhQAYXQNcM5"
      },
      "source": [
        "## Evaluate System on the Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0YBImjeNhuJ"
      },
      "source": [
        "Q14  Evaluate the final model on the test set.  Use the Linear Regression model for the final testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hPyYHOkwITQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "V8T7TFrY9eRM"
      },
      "source": [
        "#@title\n",
        "final_model = lin_reg\n",
        "\n",
        "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
        "y_test = strat_test_set[\"median_house_value\"].copy()\n",
        "\n",
        "X_test_prepared = full_pipeline.transform(X_test)\n",
        "final_predictions = final_model.predict(X_test_prepared)\n",
        "\n",
        "final_mse = mean_squared_error(y_test, final_predictions)\n",
        "final_rmse = np.sqrt(final_mse)\n",
        "final_rmse"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}