{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti103/blob/master/session-4/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_Mm98QgB81p"
   },
   "source": [
    "# Classification\n",
    "\n",
    "We have worked with regression type of problem in the previous exercise. Let us now look at classification type of problem.  We will start with a simpler binary classification problem (i.e. we are only dealing with prediction two classes, e.g. 0 or 1, False or True, Negative or Positive, etc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQxuTZ8NB81v"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "We will use a relatively small dataset from UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. https://goo.gl/U2Uwz2\n",
    "\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. Based on the features extracted, each sample is classified as  malignant (labelled as 0) or benign (labelled as 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the text label of the target\n",
    "print(cancer.target_names)\n",
    "\n",
    "# print out the unique values of the target\n",
    "print(np.unique(cancer.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Based on the output of the above cell, what does a target label of 0 and 1 corresponds to? \n",
    "\n",
    "\n",
    "<details>\n",
    " <summary>Click here for answer</summary>\n",
    "  label 0 refers to malignant, and label 1 refers to benign\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our X (features) and y (label) based on the dictionary keys above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oEz--sQzB81z",
    "outputId": "7351dcd7-9b01-40bc-e65c-e750096c80dc"
   },
   "outputs": [],
   "source": [
    "X, y = cancer[\"data\"], cancer[\"target\"]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the shape of X, that there are 30 different features used to classify the sample. \n",
    "\n",
    "Let's see how many are labelled as malignant and as benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nOJTqlAtB811",
    "outputId": "b8e4f6a2-7ede-449e-ebff-5e221c3b4fb0"
   },
   "outputs": [],
   "source": [
    "num_malignant = np.sum(y==0)\n",
    "num_benign = np.sum(y==1)\n",
    "print('malignant = {}'.format(num_malignant))\n",
    "print('benign = {}'.format(num_benign))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAnKgcbFAsQ5"
   },
   "source": [
    "Split the data into training set and test set (with 80:20 ratio) and shuffle the data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtvNaJVEB82P"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C0VCtuUFJRFd"
   },
   "source": [
    "**Question 2**\n",
    "\n",
    "Create a binary classifier capable of distinguishing between malignant and benign breast mass sample.  \n",
    "\n",
    "* Use Logistic Regression and train it on the whole training set. (use liblinear as solver and 42 as random_state)\n",
    "* Use the trained classifier to predict the test set \n",
    "* Calculate the accuracy score \n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Click here for answer</summary>\n",
    "    \n",
    "``` python\n",
    "    \n",
    "lr_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "    \n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "bEA7b9G0B82W",
    "outputId": "f43d1abf-2152-4a54-868e-ad8574971470"
   },
   "outputs": [],
   "source": [
    "# import the logistic regressor \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate a logistic regressor using liblinear as solver\n",
    "\n",
    "lr_clf = None\n",
    "\n",
    "# train on the X_train \n",
    "\n",
    "\n",
    "# make prediction on test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy on the chosen test set seems quite decent. But how do we know if it is because we are lucky to pick a 'easy' test set. Since our test set is pretty small, it may not be an accurate reflection of the accuracy of our model. A better way is to use cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "og2sJ4NWHWrc"
   },
   "source": [
    "### Measuring Accuracy using Cross-Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQae1eJ7H23i"
   },
   "source": [
    "**Question 3**\n",
    "\n",
    "Evaluate the **accuracy** of the model using cross-validation on the **train** data set with the `cross_val_score()` function, with 3 folds. What do you observe? \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "``` python \n",
    "    \n",
    "cross_val_score(lr_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
    "    \n",
    "```\n",
    "<br/>\n",
    "There are 3 different accuracy scores reported, one for each fold. \n",
    "The accuracy score also differs for each fold\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HYv1hPCXB82b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Complete your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1N-vEQLLf07"
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "A much better way to understand how a trained classifier perform is to look at the confusion matrix.  \n",
    "\n",
    "\n",
    "*   Generate a set of predictions using `cross_val_predict()` on the train data set\n",
    "*   Compute the confusion matrix using the `confusion_matrix()` function \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "``` python\n",
    "    \n",
    "y_train_pred = cross_val_predict(lr_clf, X_train, y_train, cv=3)\n",
    "    \n",
    "confusion_matrix(y_train, y_train_pred) \n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIsG4HJvB82l"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Complete your code here \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_s0_VWbRB82n"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# complete your code here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fl49kQe0NCXS"
   },
   "source": [
    "A perfect classifier would have only true positives and true negatives, so its confusion matrix would have non zero values only on its main diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xAZAsN8jB82p"
   },
   "outputs": [],
   "source": [
    "y_train_perfect_predictions = y_train  # pretend we reached perfection\n",
    "\n",
    "confusion_matrix(y_train, y_train_perfect_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "How do we know which row of the confusion matrix corresponds to which label? \n",
    "\n",
    "*Hint*: use the the `classes_` attribute of the classifier.\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "``` python\n",
    "    \n",
    "lr_clf.classes_\n",
    "    \n",
    "```\n",
    "<br/>  \n",
    "The ordering of the different classes will be the order the confusion matrix shows the different classes, the 1st row corresponds to 1st class, 2nd row corresponds to 2nd class, and so on. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P3eFi0FnNhIO"
   },
   "source": [
    "### Precision and Recall\n",
    "\n",
    "**Question 6**\n",
    "\n",
    "From the confusion matrix above, compute the precision, recall and F1 score **manually** using the following formula:\n",
    "\n",
    "- `recall = TP/(TP+FN)`\n",
    "- `precision = TP/(TP+FP)`\n",
    "- `F1 = 2*precision*recall/(precision + recall)`\n",
    "\n",
    "Since we are doing breast cancer detection, a **positive** test result is considered as maglinant, while a **negative** is considered as benign.\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "Earlier on, we have determined that 0 is malignant and 1 is benign. So, our positive label is 0 and negative label is 1. \n",
    "    \n",
    "From the confusion matrix, we can obtain the following: \n",
    "- TP = 156\n",
    "- FN = 13\n",
    "- FP = 10 \n",
    "- TN = 276\n",
    "\n",
    "Now we can calculate recall, precision, and f1 easily: \n",
    "\n",
    "- recall = 156/(156+13) = 0.92\n",
    "- precision = 156/(156+10) = 0.94\n",
    "- f1 = 0.93\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete your code here \n",
    "\n",
    "recall = None \n",
    "\n",
    "precision = None\n",
    "\n",
    "f1 = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**\n",
    "\n",
    "Now use the scikit learn's metric function to compute recall, precision and f1_score and compare the values with those manually computed: \n",
    "- recall_score()\n",
    "- precision_score()\n",
    "- f1_score()\n",
    "\n",
    "Are they the same? If your calculation is different from those computed by the scikit-learn, why? \n",
    "\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "\n",
    "``` python\n",
    "    \n",
    "recall_score(y_train, y_train_pred)\n",
    "precision_score(y_train, y_train_pred)\n",
    "f1_score(y_train, y_train_pred)\n",
    "    \n",
    "```\n",
    "<br/>\n",
    "If you use the codes above, they would have given you different results from what you have computed earlier. \n",
    "    \n",
    "It is because, be default, they will treat 1 as positive label and 0 as negative label. To change the default, we need to use *pos_label* to specify what is considered as positive label:\n",
    "    \n",
    "``` python\n",
    "    \n",
    "recall_score(y_train, y_train_pred, pos_label=0)\n",
    "precision_score(y_train, y_train_pred, pos_label=0)\n",
    "f1_score(y_train, y_train_pred, pos_label=0)\n",
    "   \n",
    "```\n",
    "</details>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPGew5oZB82s"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "# Complete your code here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The is a another useful function called `classification_report()` in scikit-learn that gives all the metrics in one glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have different precison and recall scores for each class (0 and 1). \n",
    "\n",
    "**Question 8**\n",
    "\n",
    "Also note that we have slightly different averages for precision, recall and f1 : macro average and weighted average in the classication_report. What is the difference between the two ? You can refer to this [link](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report) for info.  Manually calculate the macro and weighted average to check your understanding. \n",
    "\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "```python\n",
    "    \n",
    "macro_average_recall = (0.923 + 0.965)/2 \n",
    "weighted_average_recall = (0.923*169 + 0.965*286)/455\n",
    "    \n",
    "```\n",
    "<br/>\n",
    "\n",
    "Note: Here we use more decimal places than what was shown in the classification table to obtain the final numbers. \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete your code here \n",
    "\n",
    "\n",
    "macro_average_recall = None \n",
    "weighted_average_recall = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wt3c3e2GqKbF"
   },
   "source": [
    "### Precision and Recall tradeoff\n",
    "\n",
    "The confusion matrix and the classification report provide a very detailed analysis of\n",
    "a particular set of predictions. However, the predictions themselves already threw\n",
    "away a lot of information that is contained in the model. \n",
    "\n",
    "Most classifiers provide a `decision_function()` or a `predict_proba()` method to\n",
    "assess degrees of certainty about predictions. Making predictions can be seen as\n",
    "thresholding the output of decision_function or predict_proba at a certain fixed\n",
    "pointâ€”in binary classification we use 0 for the decision function and 0.5 for\n",
    "predict_proba.\n",
    "\n",
    "In logistic regression, we can use the `decision_function()` method to compute the scores.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Lk7wPC5gB828",
    "outputId": "f239d0d6-513a-4738-d2bb-19971d489526"
   },
   "outputs": [],
   "source": [
    "sample_X = X[20]\n",
    "sample_y = y[20]\n",
    "\n",
    "y_score = lr_clf.decision_function([sample_X])\n",
    "print(y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "247XTW-VT2s2"
   },
   "source": [
    "With threshold = 0, the prediction (of positive case, i.e. 1) is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1uGbDVN7B82_",
    "outputId": "1c0ef452-64cd-45fb-9403-35d51266c3c4"
   },
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "y_some_X_pred = (y_score > threshold)\n",
    "print(y_some_X_pred == sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FnFq_IupT9xp"
   },
   "source": [
    "With threshold set at 6, prediction (of positive case, i.e. 1) is wrong. In other words, we failed to detect positive cases (lower recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t_nQQ48UB83G",
    "outputId": "ca3875f4-17c4-4ea0-9289-f016e6668c8e"
   },
   "outputs": [],
   "source": [
    "threshold = 6\n",
    "y_some_data_pred = (y_score > threshold)\n",
    "print(y_some_data_pred == sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNuRTKNbUFTF"
   },
   "source": [
    "With a higher threshold, it decreases the recall and increases the precision. Conversely, with a lower threshold, we increases recall at the expense of decrease in precision. To decide which threshold to use, get the scores of all instances in the training set using the `cross_val_predict()` function to return decision scores instead of predictions.\n",
    "\n",
    "Perform cross validation to get the scores for all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gYak-Q24B83I"
   },
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(lr_clf, X_train, y_train, cv=3,\n",
    "                             method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bBILPUtFUL1j"
   },
   "source": [
    "Compute precision and recall for all possible thresholds using the precision_recall_curve function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHkO1oNeB83L"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azObPxIkB83O"
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.legend(loc=\"center right\", fontsize=16) \n",
    "    plt.xlabel(\"Threshold\", fontsize=16)        \n",
    "    plt.grid(True)                                           \n",
    "\n",
    "plt.figure(figsize=(8, 4))                      \n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set our threshold and use it to make predictions, we will get the same prediction results as the `cross_val_predict()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YfZlPm4qB83T",
    "outputId": "57954243-3d7a-42e6-9766-39d42488321e"
   },
   "outputs": [],
   "source": [
    "(y_train_pred == (y_scores > 0)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to select a good precision/recall trade-off is to plot precision directly against recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6FpiXz4B83X"
   },
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    #plt.axis([-5, 10, 0, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R7AaLbqHUng3"
   },
   "source": [
    "We want to aim for 98% or better precision, compute the threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4Y1-4NknB83c",
    "outputId": "69869f53-a423-4cca-e470-6b6877f5cd99"
   },
   "outputs": [],
   "source": [
    "threshold_98_precision = thresholds[np.argmax(precisions >= 0.98)]\n",
    "threshold_98_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T69AzgsLB83i"
   },
   "outputs": [],
   "source": [
    "y_train_pred_98 = (y_scores >= threshold_98_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-44nclYVJQa"
   },
   "source": [
    "Compute the precision and recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xSwNkEYiB83k",
    "outputId": "24f4bfba-1791-4e7d-9368-ecd34f5dc83e"
   },
   "outputs": [],
   "source": [
    "precision_score(y_train, y_train_pred_98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8c1kabSUB83n",
    "outputId": "0d2600e3-4c7e-476b-b53a-49893ed07347"
   },
   "outputs": [],
   "source": [
    "recall_score(y_train, y_train_pred_98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQNRFi0psX4-"
   },
   "source": [
    "### ROC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NBOH7i-AVVls"
   },
   "source": [
    "The receiver operation characteristic (ROC) curve is another common tool used with binary classifiers.  It is similar to the precision/recall curve, but it plots the true positive rate (recall) against the false positive rate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vFMPnJ8gVou3"
   },
   "source": [
    "**Question 9**\n",
    "\n",
    "Compute the True positive rate (TPR), False positive rate (FPR) for various thresholds using the `roc_curve()` function.\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "    \n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gl2oIIJwsrZU"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Complete your code here \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "colab_type": "code",
    "id": "XRx8OqyRssRt",
    "outputId": "225df059-de74-4fc2-abe2-34159c395083"
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])                                    \n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) \n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    \n",
    "    plt.grid(True)                                            \n",
    "\n",
    "plt.figure(figsize=(8, 6))                        \n",
    "plot_roc_curve(fpr, tpr)        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9CZ_n_oGWTmn"
   },
   "source": [
    "The higher the recall (TPR), the more false positives (FPR) the classifier produces.  The dotted line represents the ROC curve of a purely random classifier, a good classfier stays as far away from the line as possible.\n",
    "\n",
    "**Quesiton 10**\n",
    "\n",
    "Compute the area under the curve (AUC) using `roc_auc_score()`\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "    \n",
    "roc_auc_score(y_train, y_scores)\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "I71PFhdwsy3n",
    "outputId": "2210192b-0814-439b-b2e7-5bbe77421b14"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Complete your code here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11**\n",
    "\n",
    "We are finally done with our binary classification...Wait a minute! Did we just computed all the evaluation metrics on ***training set*** ??!!  Isn't it bad practice to do so.. Don't we need to use ***test set*** to evaluate how good is our model?\n",
    "\n",
    "Why?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "We only evaluate our model after we are satisfied with performance of it on our validation set. We will do our model fine-tuning on the validation set and not test set. In our case, since our training set is pretty small (only about 400+), if we are to set aside a validation set, then our training set would be too small. That is why we use cross_validation to evaluate our model\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1K1uPeXrB837"
   },
   "source": [
    "## Multiclass classification\n",
    "\n",
    "We will now look at multi-class classification. The dataset we are going to use is the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
    "\n",
    "The data set contains images of hand-written digits: 10 classes where each class refers to a digit. Each digit is a 8x8 image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "print(digits.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12**\n",
    "\n",
    "Now create the X (the features) and y (the label) from the digits dataset.  X is a np.array of 64 pixel values, while y is the label e.g. 0, 1, 2, 3, .. 9.\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "```python\n",
    "    \n",
    "X = digits['data']\n",
    "y = digits['target']\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete your code here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the image of a particular digit to visualize it.  Before plotting, we need to reshape the 64 numbers into 8 x 8 image arrays so that it can be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's choose any one of the row and plot it\n",
    "some_digit = X[100]\n",
    "\n",
    "# print out the corresponding label\n",
    "print('digit is {}'.format(y[100]))\n",
    "\n",
    "# reshape it to 8 x 8 image\n",
    "some_digit_image = some_digit.reshape(8, 8)\n",
    "\n",
    "plt.imshow(some_digit_image, cmap = mpl.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 13**\n",
    "\n",
    "Split the data into train and test set, and randomly shuffle the data.\n",
    "\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True, random_state=42)\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete your code here\n",
    "\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7SaIkTTXM7Q"
   },
   "source": [
    "Multiclass classifiers distinguish between more than two classess.  Scikit-learn detects when you try to use a binary classification algorithm for a multiple class classification task and it automatically runs one-versus-all (OvA)\n",
    "\n",
    "**Question 14**\n",
    "\n",
    "Use Logistic Regression to train using the training set, and make a prediction of the chosen digit (`some_digit`). Is the prediction correct?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "lr_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFp4PXurB838"
   },
   "outputs": [],
   "source": [
    "# Complete the code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yMFdDeHqYZ81"
   },
   "source": [
    "Under the hood, Scikit-Learn actually trained 10 binary classifiers, got their decision scores for the image and selected the class with the highest score.  \n",
    "\n",
    "**Question 15**\n",
    "\n",
    "Compute the scores for `some_digit` using the `decision_function()` method to return 10 scores, one per class.\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "    \n",
    "y_scores = lr_clf.decision_function([some_digit])\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cpNVjIuLB84A"
   },
   "outputs": [],
   "source": [
    "# complete the code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNlx7EgOZFN7"
   },
   "source": [
    "The highest score is the one corresponding to the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4SuHkxLB84C"
   },
   "outputs": [],
   "source": [
    "index = np.argmax(some_digit_scores)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlPa3cdPB84E"
   },
   "outputs": [],
   "source": [
    "lr_clf.classes_[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZkNaCi4uZqbr"
   },
   "source": [
    "**Question 16**\n",
    "\n",
    "Use `cross_val_score()` to evaluate the classifier\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "```python \n",
    "    \n",
    "cross_val_score(lr_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
    "    \n",
    "```\n",
    "</details>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9IlBdvZB84V"
   },
   "outputs": [],
   "source": [
    "# Complete your code here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X4JJ4J2mabyg"
   },
   "source": [
    "**Question 17**\n",
    "\n",
    "Compute the confusion matrix of the classifier. From the confusion matrix, which two digits tend to be confused with each other?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "```python \n",
    "\n",
    "y_train_pred = cross_val_predict(lr_clf, X_train, y_train, cv=3)\n",
    "confusion_matrix(y_train, y_train_pred)\n",
    "    \n",
    "```\n",
    "<br/>\n",
    "1 and 8 are confused with each other. \n",
    "    \n",
    "</details>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-6kiojKB84b"
   },
   "source": [
    "**Question 13**\n",
    "\n",
    "Print out the classification_report.  \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "```python \n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "    \n",
    "```\n",
    "</details>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete your code here \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "4 Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
