{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4 Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti103/blob/master/session-4/classification_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_Mm98QgB81p"
      },
      "source": [
        "# Classification\n",
        "\n",
        "We have worked with regression type of problem in the previous exercise. Let us now take a closer look at classification type of problem.  \n",
        "\n",
        "We will work with both binary classification and multi-class classification problems, and learn to compute different metrics to evaluate a classification model. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmlRk9r2h_wi"
      },
      "source": [
        "## Binary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQxuTZ8NB81v"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "We will be using an SMS spam/ham dataset and build a binary classification model to help us predict if a text message is a spam or not. \n",
        "\n",
        "Let's go head and load the data into a panda dataframe and look at the first few samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thUdLD3Bh_wj"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/smsspamcollection.tsv'\n",
        "df = pd.read_csv(data_url, sep='\\t')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NblFwdE7h_wk"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY9s0va6h_wk"
      },
      "source": [
        "Let's see what are the different labels we have. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX0n0Illh_wk"
      },
      "source": [
        "df['label'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMcy1sToh_wl"
      },
      "source": [
        "You will notice that we have two different labels: 'ham' and 'spam', both a text string (dtype=object)\n",
        "\n",
        "As most of the evaluation metrics in scikit-learn assume (by default) positive label as 1 and negative label as 0, for convenience, we will first convert the label to 1 and 0. As our task is to detect spam, the positive label (label 1) in our case will be for spam.  \n",
        "\n",
        "Let's create a mapping to map the string label to its corresponding numeric label and use the pandas [map()](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html)  function to change the label to numeric label. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jBFjrCvh_wl"
      },
      "source": [
        "labelmap = { 'ham': 0, 'spam':1}\n",
        "\n",
        "df['label'] = df['label'].map(labelmap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OaAHrldh_wl"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z71DEMch_wm"
      },
      "source": [
        "Always a good practice to check if there is any missing values, using ``isnull()`` method of dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdGAtJHah_wm"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmHQdxI1h_wm"
      },
      "source": [
        "Let's get a sense of the distribution of positive and negative cases to see if we are dealing with imbalanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vydxvn-8h_wm"
      },
      "source": [
        "df['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iis25-ZZh_wn"
      },
      "source": [
        "You will see that we have a lot more 'ham' messages than 'spam' messages: 4825 out of 5572 messages, or 86.6%, are ham. This means that any text classification model we create has to perform **better than 86.6%** to beat random chance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJjIXqWgh_wn"
      },
      "source": [
        "### Split data into train and test set\n",
        "\n",
        "We will have to first decide what we want to use as features. For this lab, let us just start simply, only use the text message and ignore others like punctuation and message length. \n",
        "\n",
        "We then split the data randomly into 80-20 split of train and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BhOVPHgh_wn"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['message']  # this time we want to look at the text\n",
        "y = df['label'].to_numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03q4HTyJh_wo"
      },
      "source": [
        "### Text Pre-processing \n",
        "\n",
        "We cannot use text string directly as our input features for training our model. It has to be converted into numeric features first. There are many ways to do this, from simple bag-of-words approach to more sophisticated dense embedding using modern neural model. \n",
        "\n",
        "In this example, we will use the TF-IDF to represent our string as numeric vector. Text usually has to be pre-processed first, for example removal of punctuation marks, stop words, lower-casing, etc, before convert to numeric vector. Scikit-learn's [TFIDFVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class conveniently do all these for us, transforming our collection of text into document matrix.\n",
        "\n",
        "By default TfidfVectorizer will lowercase the text and remove punctuation. We have also removed the English stop_words such as 'the', 'is', etc. and also specify that only words that occurs 2 times or more should be included as part of the vocabulary (min_df=2). By keeping our vocubalary small, we are keeping our number of features small. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWdntpq4h_wo"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        " \n",
        "tfidf_vect = TfidfVectorizer(stop_words='english', min_df=2)\n",
        "\n",
        "# We will first fit the vectorizer to the training text, \n",
        "# and transform the training text into dcoument matrix\n",
        "X_train_vect = tfidf_vect.fit_transform(X_train)\n",
        "print(X_train_vect.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw9lWEDAh_wo"
      },
      "source": [
        "You can print out the vocabulary learnt by the TfidfVectorizer by accessing the instance variable `vocabulary_`. Notice that the vocbulary size is the feature size of your vectorized X_train. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHQxJtS2h_wp"
      },
      "source": [
        "## printout a subset of vocabulary\n",
        "print('Vocabulary size : ', len(tfidf_vect.vocabulary_))\n",
        "print('Some words in the vocab : \\n',  list(tfidf_vect.vocabulary_.items())[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyePio4hh_wp"
      },
      "source": [
        "We will need to transform our X_test as well. We will use the TfidfVectorizer already fitted on train data to transform. There maybe a chance that certain words in the test set are not found in the vocabulary derived from the train set. In this case, the TfidfVectorizer will just ignore the unknown words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWTIy-lJh_wp"
      },
      "source": [
        "X_test_vect = tfidf_vect.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1tc8g_mh_wp"
      },
      "source": [
        "Now we have gotten our features. Let's go ahead and train our model! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0VCtuUFJRFd"
      },
      "source": [
        "## Train a classifier \n",
        "\n",
        "We will now train a binary classifier capable of distinguishing between ham and spam. \n",
        "\n",
        "* Use Logistic Regression and train it on the whole training set. (use liblinear as solver and 42 as random_state)\n",
        "* Use the trained classifier to predict the test set \n",
        "* Calculate the accuracy score "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEA7b9G0B82W"
      },
      "source": [
        "# import the logistic regressor \n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "lr_clf.fit(X_train_vect, y_train)\n",
        "y_pred = lr_clf.predict(X_test_vect)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikN0OXQ_h_wq"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPIe40BLh_wq"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svc = LinearSVC(random_state=42)\n",
        "svc.fit(X_train_vect, y_train)\n",
        "y_pred = svc.predict(X_test_vect)\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byC9LSHsh_wq"
      },
      "source": [
        "Our accuracy on the chosen test set seems quite decent. But how do we know if it is because we are lucky to pick a 'easy' test set. Since our test set is pretty small, it may not be an accurate reflection of the accuracy of our model. A better way is to use cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og2sJ4NWHWrc"
      },
      "source": [
        "### Measuring Accuracy using Cross-Validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQae1eJ7H23i"
      },
      "source": [
        "\n",
        "Evaluate the **accuracy** of the model using cross-validation on the **train** data set with the `cross_val_score()` function, with 5 folds. \n",
        "\n",
        "**Exercise 1:**\n",
        "\n",
        "What do you observe? What is the average validation accuracy?\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "    \n",
        "val_accuracies = cross_val_score(lr_clf, X_train_vect, y_train, cv=5, scoring=\"accuracy\")\n",
        "print(val_accuracies)\n",
        "print(np.mean(val_accuracies))\n",
        "    \n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYv1hPCXB82b"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Complete your code here \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1N-vEQLLf07"
      },
      "source": [
        "### Confusion Matrix\n",
        "\n",
        "\n",
        "A much better way to understand how a trained classifier perform is to look at the confusion matrix. We will do the following: \n",
        "*   Generate a set of predictions using `cross_val_predict()` on the train data set\n",
        "*   Compute the confusion matrix using the `confusion_matrix()` function.  Use ConfusionMatrixDisplay to plot the confusion matrix graphically.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIsG4HJvB82l"
      },
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "y_train_pred = cross_val_predict(lr_clf, X_train_vect, y_train, cv=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s0_VWbRB82n"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(y_train, y_train_pred) \n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=lr_clf.classes_)\n",
        "disp.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrDcSqkTh_ws"
      },
      "source": [
        "**Exercise 2:**\n",
        "\n",
        "What can you tell from the confusion matrix? What kind of errors does the model more frequently make? \n",
        "<br/>\n",
        "<details><summary>Click here for answer</summary>\n",
        "It predicts 400 spam messages correctly but got 198 wrong, represents only 66.8% recall rate for 'spam' class. It did however, better at predicting ham messages, which is not suprising, given we have a lot more ham messages in our training set.\n",
        "\n",
        "<p><br/>\n",
        "Important lesson here: Just looking at accuracy alone will not give you a full picture of the performance of your model. \n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3eFi0FnNhIO"
      },
      "source": [
        "### Precision and Recall\n",
        "\n",
        "**Exercise 3:**\n",
        "\n",
        "From the confusion matrix above, compute the precision, recall and F1 score **manually** using the following formula:\n",
        "\n",
        "- `recall = TP/(TP+FN)`\n",
        "- `precision = TP/(TP+FP)`\n",
        "- `F1 = 2*precision*recall/(precision + recall)`\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "    \n",
        "By convention, we use label 1 as positive case and label 0 as negative case. \n",
        "    \n",
        "From the confusion matrix, we can obtain the following: \n",
        "- TP = 400\n",
        "- FN = 198\n",
        "- FP = 9\n",
        "- TN = 3850\n",
        "\n",
        "Now we can calculate recall, precision, and f1 easily: \n",
        "\n",
        "- recall = TP/(TP+FN) = 400/(400+198) = 0.67\n",
        "- precision = TP/(TP+FP) = 400/(400+9) = 0.98\n",
        "- f1 = 2\\*precision\\*recall/(precision+recall) = 0.8\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zj-8c7wh_wt"
      },
      "source": [
        "Now we use the scikit learn's metric function to compute recall, precision and f1_score and compare the values with those manually computed: \n",
        "- recall_score()\n",
        "- precision_score()\n",
        "- f1_score()\n",
        "\n",
        "Are they the same as your calculation? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPGew5oZB82s"
      },
      "source": [
        "from sklearn.metrics import recall_score, precision_score, f1_score\n",
        "\n",
        "print(recall_score(y_train, y_train_pred))\n",
        "print(precision_score(y_train, y_train_pred))\n",
        "print(f1_score(y_train, y_train_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoXlHP26h_wt"
      },
      "source": [
        "The is a another useful function called `classification_report()` in scikit-learn that gives all the metrics in one glance. Note that the ``classification_report()`` provides the precision/recall/f1-score values for each of the class. \n",
        "\n",
        "Note that we have different precison and recall scores for each class (0 and 1). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inLIsQbTh_wu"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_train, y_train_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF-hsg0Dh_wu"
      },
      "source": [
        "Also note that we have different averages for precision, recall and f1 : macro average and weighted average in the classication_report. What is the difference between the two ? You can refer to this [link](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report) for info.  Manually calculate the macro and weighted average to check your understanding. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt3c3e2GqKbF"
      },
      "source": [
        "### Precision and Recall tradeoff\n",
        "\n",
        "The confusion matrix and the classification report provide a very detailed analysis of\n",
        "a particular set of predictions. However, the predictions themselves already threw\n",
        "away a lot of information that is contained in the model. \n",
        "\n",
        "Most classifiers provide a `decision_function()` or a `predict_proba()` method to\n",
        "assess degrees of certainty about predictions. Making predictions can be seen as\n",
        "thresholding the output of decision_function or predict_proba at a certain fixed\n",
        "pointâ€” in binary classification we use 0 for the decision function and 0.5 for\n",
        "predict_proba.\n",
        "\n",
        "In logistic regression, we can use the `decision_function()` method to compute the scores.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCaRVdkmh_wu"
      },
      "source": [
        "First let's find a positive sample (using ``np.where`` to find all samples where y label == 1, and uses the first result as sample) and examine the decision score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kpdT5lUh_wv"
      },
      "source": [
        "idx = np.where(y_train == 1)[0][0]\n",
        "print(idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk7wPC5gB828"
      },
      "source": [
        "sample_X = X_train_vect[idx]\n",
        "sample_y = y_train[idx]\n",
        "\n",
        "y_score = lr_clf.decision_function(sample_X)\n",
        "print(y_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "247XTW-VT2s2"
      },
      "source": [
        "With threshold = 0, the prediction (of positive case, i.e. 1) is correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uGbDVN7B82_"
      },
      "source": [
        "threshold = 0\n",
        "y_some_X_pred = (y_score > threshold)\n",
        "print(y_some_X_pred == sample_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnFq_IupT9xp"
      },
      "source": [
        "With threshold set at 6, prediction (of positive case, i.e. 1) is wrong. In other words, we failed to detect positive cases (lower recall)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_nQQ48UB83G"
      },
      "source": [
        "threshold = 6\n",
        "y_some_data_pred = (y_score > threshold)\n",
        "print(y_some_data_pred == sample_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNuRTKNbUFTF"
      },
      "source": [
        "With a higher threshold, it decreases the recall and increases the precision. Conversely, with a lower threshold, we increases recall at the expense of decrease in precision. To decide which threshold to use, get the scores of all instances in the training set using the `cross_val_predict()` function to return decision scores instead of predictions.\n",
        "\n",
        "Perform cross validation to get the scores for all instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYak-Q24B83I"
      },
      "source": [
        "y_scores = cross_val_predict(lr_clf, X_train_vect, y_train, cv=5,\n",
        "                             method=\"decision_function\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBILPUtFUL1j"
      },
      "source": [
        "Compute precision and recall for all possible thresholds using the precision_recall_curve function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHkO1oNeB83L"
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azObPxIkB83O"
      },
      "source": [
        "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
        "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
        "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
        "    plt.legend(loc=\"center right\", fontsize=16) \n",
        "    plt.xlabel(\"Threshold\", fontsize=16)        \n",
        "    plt.grid(True)                                           \n",
        "\n",
        "plt.figure(figsize=(8, 4))                      \n",
        "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7PxKzGCh_wz"
      },
      "source": [
        "If we set our threshold and use it to make predictions, we will get the same prediction results as the `cross_val_predict()` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfZlPm4qB83T"
      },
      "source": [
        "(y_train_pred == (y_scores > 0)).all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPMfZGQ7h_wz"
      },
      "source": [
        "Another way to select a good precision/recall trade-off is to plot precision directly against recall."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6FpiXz4B83X"
      },
      "source": [
        "def plot_precision_vs_recall(precisions, recalls):\n",
        "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
        "    plt.xlabel(\"Recall\", fontsize=16)\n",
        "    plt.ylabel(\"Precision\", fontsize=16)\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_precision_vs_recall(precisions, recalls)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7AaLbqHUng3"
      },
      "source": [
        "We want to aim for 80% or better recall, compute the threshold value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y1-4NknB83c"
      },
      "source": [
        "threshold_80_recall = thresholds[np.argmin(recalls >= 0.8)]\n",
        "threshold_80_recall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T69AzgsLB83i"
      },
      "source": [
        "y_train_pred_80 = (y_scores >= threshold_80_recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-44nclYVJQa"
      },
      "source": [
        "Compute the precision and recall score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSwNkEYiB83k"
      },
      "source": [
        "precision_score(y_train, y_train_pred_80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c1kabSUB83n"
      },
      "source": [
        "recall_score(y_train, y_train_pred_80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNUJlvvQh_w0"
      },
      "source": [
        "print(classification_report(y_train, y_train_pred_80))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQNRFi0psX4-"
      },
      "source": [
        "### ROC Curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBOH7i-AVVls"
      },
      "source": [
        "The receiver operation characteristic (ROC) curve is another common tool used with binary classifiers.  It is similar to the precision/recall curve, but it plots the true positive rate (recall) against the false positive rate.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl2oIIJwsrZU"
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_train, y_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRx8OqyRssRt"
      },
      "source": [
        "def plot_roc_curve(fpr, tpr, label=None):\n",
        "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
        "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
        "    plt.axis([0, 1, 0, 1])                                    \n",
        "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) \n",
        "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    \n",
        "    plt.grid(True)                                            \n",
        "\n",
        "plt.figure(figsize=(8, 6))                        \n",
        "plot_roc_curve(fpr, tpr)        \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CZ_n_oGWTmn"
      },
      "source": [
        "The higher the recall (TPR), the more false positives (FPR) the classifier produces.  The dotted line represents the ROC curve of a purely random classifier, a good classfier stays as far away from the line as possible.\n",
        "\n",
        "Let's Compute the area under the curve (AUC) using `roc_auc_score()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I71PFhdwsy3n"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "roc_auc_score(y_train, y_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q_IzaC4h_w1"
      },
      "source": [
        "**Exercise 4:**\n",
        "\n",
        "We are finally done with our binary classification...Wait a minute! Did we just computed all the evaluation metrics on ***training set*** ??!!  Isn't it bad practice to do so.. Don't we need to use ***test set*** to evaluate how good is our model?\n",
        "\n",
        "Why?\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "We only evaluate our model after we are satisfied with performance of it on our validation set. We will do our model fine-tuning on the validation set and not test set. In our case, since our training set is pretty small, if we are to set aside a validation set, then our training set would be too small. That is why we use ``cross_validation`` to evaluate our model\n",
        "    \n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86FVXe1Dh_w1"
      },
      "source": [
        "lr_clf.fit(X_train_vect, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCDELtexh_w1"
      },
      "source": [
        "lr_clf.score(X_test_vect, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K1uPeXrB837"
      },
      "source": [
        "## Multiclass classification\n",
        "\n",
        "We will now look at multi-class classification. The dataset we are going to use is the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
        "\n",
        "The data set contains images of hand-written digits: 10 classes where each class refers to a digit. Each digit is a 8x8 image.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52O2quRuh_w2"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "\n",
        "print(digits.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFYZigJxh_w2"
      },
      "source": [
        "**Exercise 5:**\n",
        "\n",
        "Now create the X (the features) and y (the label) from the digits dataset.  X is a np.array of 64 pixel values, while y is the label e.g. 0, 1, 2, 3, .. 9.\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "    \n",
        "```python\n",
        "    \n",
        "X = digits['data']\n",
        "y = digits['target']\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_BkntaPh_w2"
      },
      "source": [
        "# Complete your code here \n",
        "\n",
        "X = None\n",
        "y = None"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwTrkEd0h_w2"
      },
      "source": [
        "Let's plot the image of a particular digit to visualize it.  Before plotting, we need to reshape the 64 numbers into 8 x 8 image arrays so that it can be plotted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAYtlCNlh_w2"
      },
      "source": [
        "import matplotlib as mpl\n",
        "\n",
        "\n",
        "# let's choose any one of the row and plot it\n",
        "some_digit = X[100]\n",
        "\n",
        "# print out the corresponding label\n",
        "print('digit is {}'.format(y[100]))\n",
        "\n",
        "# reshape it to 8 x 8 image\n",
        "some_digit_image = some_digit.reshape(8, 8)\n",
        "\n",
        "plt.imshow(some_digit_image, cmap = mpl.cm.binary, interpolation=\"nearest\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOo2qyvmh_w3"
      },
      "source": [
        "**Exercise 6**\n",
        "\n",
        "Split the data into train and test set, and randomly shuffle the data.\n",
        "\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "    \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True, random_state=42)\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC5_m3I_h_w3"
      },
      "source": [
        "## Complete your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7SaIkTTXM7Q"
      },
      "source": [
        "Multiclass classifiers distinguish between more than two classess.  Scikit-learn detects when you try to use a binary classification algorithm for a multiple class classification task and it automatically runs one-versus-all (OvA)\n",
        "\n",
        "**Exercise 7**\n",
        "\n",
        "Use Logistic Regression to train using the training set, and make a prediction of the chosen digit (`some_digit`). Is the prediction correct?\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "\n",
        "lr_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "lr_clf.fit(X_train, y_train)\n",
        "    \n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFp4PXurB838"
      },
      "source": [
        "# Complete the code here\n",
        "\n",
        "lr_clf = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMFdDeHqYZ81"
      },
      "source": [
        "Under the hood, Scikit-Learn actually trained 10 binary classifiers, got their decision scores for the image and selected the class with the highest score.  \n",
        "\n",
        "**Exercise 8**\n",
        "\n",
        "Compute the scores for `some_digit` using the `decision_function()` method to return 10 scores, one per class.\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "    \n",
        "some_digit_scores = lr_clf.decision_function([some_digit])\n",
        "    \n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpNVjIuLB84A"
      },
      "source": [
        "# complete the code here\n",
        "\n",
        "some_digit_scores = lr_clf.decision_function([some_digit])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "some_digit_scores"
      ],
      "metadata": {
        "id": "RezsfSax4dhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNlx7EgOZFN7"
      },
      "source": [
        "The highest score is the one corresponding to the correct class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4SuHkxLB84C"
      },
      "source": [
        "index = np.argmax(some_digit_scores)\n",
        "print(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlPa3cdPB84E"
      },
      "source": [
        "lr_clf.classes_[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkNaCi4uZqbr"
      },
      "source": [
        "**Exercise 9**\n",
        "\n",
        "Use `cross_val_score()` to evaluate the classifier's accuracy.\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "    \n",
        "```python \n",
        "    \n",
        "cross_val_score(lr_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
        "    \n",
        "```\n",
        "</details>  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9IlBdvZB84V"
      },
      "source": [
        "# Complete your code here \n",
        "\n",
        "\n",
        "cross_val_score(lr_clf, X_train, y_train, cv=5, scoring=\"accuracy\").mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4JJ4J2mabyg"
      },
      "source": [
        "**Exercise 10**\n",
        "\n",
        "Compute the confusion matrix of the classifier. From the confusion matrix, which two digits tend to be confused with each other?\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "    \n",
        "```python \n",
        "\n",
        "y_train_pred = cross_val_predict(lr_clf, X_train, y_train, cv=5)\n",
        "cm = confusion_matrix(y_train, y_train_pred)\n",
        "disp = ConfusionMatrixDisplay(cm)\n",
        "disp.plot()\n",
        "    \n",
        "```\n",
        "<br/>\n",
        "1 and 8 are confused with each other. \n",
        "    \n",
        "</details>  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7Lb2z6Sh_w5"
      },
      "source": [
        "# Complete your code here\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkdZRUxJj1pM"
      },
      "source": [
        "**Exercise 11**\n",
        "\n",
        "Print out the classification_report.  \n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "    \n",
        "```python \n",
        "\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "    \n",
        "```\n",
        "</details>  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICm0SPZZh_w5"
      },
      "source": [
        "# Complete your code here \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}