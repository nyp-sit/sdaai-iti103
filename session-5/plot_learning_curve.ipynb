{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4_cY7zsewkrP"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti103/blob/master/session-5/plot_learning_curve.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iMc4C81wkrV"
   },
   "source": [
    "# Plotting Learning Curves\n",
    "\n",
    "Welcome to learning curve programming exercise. This is part of a series of exercises to help you to acquire skills in different techniques to fine-tune your machine learning model.\n",
    "\n",
    "**You will learn how to:**\n",
    "- Dignose overfitting/underfitting problems in machine learning  \n",
    "- Plot learning curves for both classification and regression types of problems\n",
    "\n",
    "*Acknowledgement: This exercise is adapted from https://www.dataquest.io/blog/learning-curves-machine-learning/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "npRJnOmjwkrY"
   },
   "source": [
    "## 1. Import Required Packages ##\n",
    "\n",
    "Let's first import all the packages that you will need during this exercise.\n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [sklearn](http://scikit-learn.org/stable/) provides simple and efficient tools for data mining and data analysis. \n",
    "- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python.\n",
    "- [pandas](https://pandas.pydata.org) is a library for data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQxquDNVwkra"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uu_xkvFvwkrl"
   },
   "source": [
    "## 2. Learning Curve for Regression Problem ##\n",
    "First, let's get the dataset you will work on. The description of the data can be found [here](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "ncSjozEGwkro",
    "outputId": "c7efbd32-c404-4ab6-f17a-a45e80035c71"
   },
   "outputs": [],
   "source": [
    "# if you are using jupyter notebook and wish to load the data locally\n",
    "# electricity = pd.read_excel('data/combined_pp.xlsx)\n",
    "# if you wish to load data from an url\n",
    "electricity = pd.read_excel('https://github.com/nyp-sit/data/raw/master/combined_pp.xlsx')\n",
    "electricity.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_lA5FINwkrw"
   },
   "source": [
    "***Exercise***\n",
    "\n",
    "Separate the features from the target. Instantiate a LinearRegressor as the estimator to be used later \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "    \n",
    "X = electricity[features]\n",
    "y = electricity[target]\n",
    "\n",
    "estimator = LinearRegression()\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fPvU9Qjwkry"
   },
   "outputs": [],
   "source": [
    "# We separate the features and target from the data set\n",
    "features = ['AT','V','AP','RH']\n",
    "target = 'PE'\n",
    "\n",
    "### START CODE HERE ### \n",
    "X = None\n",
    "y = None \n",
    "\n",
    "# Instantiate a LinearRegressor\n",
    "estimator = None \n",
    "\n",
    "### END CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGEcloRVwkr9"
   },
   "source": [
    "`learning_curve()` in scikit-learn can be used to  generate the data (the training and validation scores) needed to plot a learning curve. The function returns a tuple containing three elements: ``train_sizes``, and ``train_scores`` and ``validation_scores``. The function accepts the following parameters:\n",
    "- estimator — indicates the learning algorithm we use to estimate the true model\n",
    "- X — the data containing the features\n",
    "- y — the data containing the target\n",
    "- train_sizes — the numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within (0, 1]. (Note: the notation (0,1] means inclusive of 0 but exclusive of 1). Otherwise it is interpreted as absolute sizes of the training sets. \n",
    "- cv — determines the cross-validation splitting strategy.\n",
    "- scoring — controls the metrics used to evaluate estimator. Possible pre-defined metrics can be found [here](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- shuffle - whether to shuffle training data before taking prefixes of it based on ``train_sizes``.\n",
    "\n",
    "You can refer to the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html) for more detail of the function. \n",
    "\n",
    "***Exercise:***\n",
    "\n",
    "Complete the code below to obtain the train and validation scores using the ``learning_curve()``. Use a cross-validation fold  of 5. You need to choose an appropriate scoring metric (in our case, `'neg_mean_squared_error'` will be a good choice) for linear regression problem. Set shuffle to ``False``.  Specify a random_state=42, so that the results across run are repeatable.\n",
    "\n",
    "There are 9568 rows of data. We set aside 80% of the data for training which is around 7654 samples. We will plot the training curve for training sizes of 1, 100, 500, 2000, 5000, 7654. \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "    \n",
    "train_sizes = [1,100,500,2000,5000,7654]\n",
    "    \n",
    "train_sizes, train_errors, validation_errors = learning_curve(\n",
    "                    estimator, X, y, train_sizes = train_sizes, cv=10,\n",
    "                    scoring = 'neg_mean_squared_error',\n",
    "                    shuffle=False, random_state=42)\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pd2Tj5lUwksA"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### \n",
    "\n",
    "# declare the list of different training sizes\n",
    "\n",
    "train_sizes = None\n",
    "\n",
    "train_sizes, train_scores, validation_scores = None\n",
    "\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ms4ETudwksH"
   },
   "source": [
    "***Exercise:***\n",
    "\n",
    "What do you think are the shapes of the train_scores and test_scores?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "<p>\n",
    "Since we specify 6 training sizes, for each training, we specify a 5-fold cross-validations, we should have 6 x 5 train_scores and test_scores.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Llc3GDOrwksK"
   },
   "source": [
    "Let us print out the values of train_scores and validation_scores (neg_mean_squared_error). Each row corresponds to a test size and each columns corresponds to a split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIvmnLM6wksM"
   },
   "outputs": [],
   "source": [
    "### Uncomment the following lines ### \n",
    "\n",
    "# print('Train scores:\\n\\n', train_errors)\n",
    "# print('\\n','-'*70)\n",
    "# print('\\nValidation scores:\\n\\n', validation_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdLtdkg5wksU"
   },
   "source": [
    "You might have noticed that some error scores on the training sets are the same. For the row corresponding to training set size of 1, this is expected, but what about other rows? With the exception of the last row, we have a lot of identical values. For instance, take the second row where we have identical values from the second split onward. Why is that so? \n",
    "\n",
    "This is caused by not randomizing the training data for each split. Let’s walk through a single example with the aid of the diagram below. When the training size is 100 the first 100 samples in the training set are selected.\n",
    "\n",
    "For the first split, these 100 samples will be taken from the second chunk. From the second split onward, these 100 samples will be taken from the first chunk. Because we don’t randomize the training set, the 100 samples used for training are the same for the second split onward. This explains the identical values from the second split onward for the 100 training instances case. The same reasoning applies to the case of training size of 500, and so on. \n",
    "\n",
    "<div>\n",
    "<img src=\"images/splits.png\" alt=\"k-fold\" width=\"600\" align='left'/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQ_eF5Y7wksW"
   },
   "source": [
    "\n",
    "***Exercise:***\n",
    "\n",
    "You can fix this problem by setting ``shuffle`` to **``True``** in the call to ``learning_curve()``.  Specify a random_state=42, so that the results across run are repeatable.\n",
    "\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "    \n",
    "train_sizes, train_errors, validation_errors = learning_curve(\n",
    "                    estimator, X, y, train_sizes = train_sizes, cv=10,\n",
    "                    scoring = 'neg_mean_squared_error',\n",
    "                    shuffle=True, random_state=42)\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jh-G6Ta-wksZ"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "train_sizes, train_errors, validation_errors = None \n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RxvZy6cawksi"
   },
   "source": [
    "To plot the learning curves, we need only a single error score per training set size, not 5. So we will take the mean values of the 5 error scores (for the 5 splits). \n",
    "You will notice that the scores (which is negative mean squared error) are negative values. We will need to negate the values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WAZrnX5wksl"
   },
   "source": [
    "***Exercise:*** \n",
    "\n",
    "Take the mean (of the 5-splits CV) of the train_errors and validation_errors and also negate (flip the sign) the mean values to get mean_squared_error (MSE) values.\n",
    "\n",
    "***Hint:*** \n",
    "\n",
    "Use the ``numpy.mean()`` function and specify the correct ``axis``.\n",
    "\n",
    "\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "train_errors_mean = -train_errors.mean(axis=1)\n",
    "validation_errors_mean = -validation_errors.mean(axis=1)\n",
    "    \n",
    "    \n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qZhDMISDwkso"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "train_errors_mean = None\n",
    "validation_errors_mean = None\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "### Uncomment the following to print out the errors ### \n",
    "# print('Mean training errors:\\n', pd.Series(train_errors_mean, index=train_sizes))\n",
    "# print('\\n', '-'*50)\n",
    "# print('Mean validation errors:\\n', pd.Series(validation_errors_mean, index=train_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoIqDGOswks0"
   },
   "source": [
    "\n",
    "**Expected Output**\n",
    "\n",
    "Mean training errors:\n",
    "<div>\n",
    "<p>\n",
    "    <table style=\"width:20%\" align=\"left\">\n",
    "      <tr>\n",
    "        <td>1</td>\n",
    "        <td>0.000000</td> \n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>100</td>\n",
    "        <td>18.022630</td> \n",
    "      </tr>\n",
    "        <td>500</td>\n",
    "        <td>19.610796</td> \n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>2000</td>\n",
    "        <td>20.668527</td> \n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>5000</td>\n",
    "        <td>20.854990</td> \n",
    "      <tr>\n",
    "      <tr>\n",
    "        <td>7654</td>\n",
    "        <td>20.817307</td> \n",
    "      </tr>\n",
    "    </table>\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJuf9IGfwks2"
   },
   "source": [
    "<div>\n",
    "<p>\n",
    "Mean validation errors:\n",
    "<table style=\"width:20%\" align=\"left\">\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>443.007852</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>100</td>\n",
    "    <td>22.096985</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>500</td>\n",
    "    <td>20.969215</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2000</td>\n",
    "    <td>20.806284</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5000</td>\n",
    "    <td>20.794057</td> \n",
    "  <tr>\n",
    "  <tr>\n",
    "    <td>7654</td>\n",
    "    <td>20.801154</td> \n",
    "  </tr>\n",
    "</table>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9Hoaigowks4"
   },
   "source": [
    "Let's define a function ``plot_curve()`` that will plot the train_errors, validation_errors against train_size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFqHrH-lwks6"
   },
   "outputs": [],
   "source": [
    "def plot_curve(title, label, train_sizes, train_scores, validation_scores, ylim=None):\n",
    "    plt.style.use('seaborn')\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)    \n",
    "    plt.xlabel(\"Training size\")\n",
    "    plt.ylabel(label)\n",
    "    plt.plot(train_sizes, train_scores, 'o-', color=\"r\",\n",
    "             label=\"Training\")\n",
    "    plt.plot(train_sizes, validation_scores, 'o-', color=\"g\",\n",
    "             label=\"Validation\")\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a3h1ttWAwktG"
   },
   "source": [
    "***Exercise:*** \n",
    "\n",
    "Plot the learning curve using the above function. You may need to limit the range of y-axis to (0,40) as the MSE for training size of 1 is very large compared to the rest, and we want to see the details of MSEs for other training sizes.\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "    \n",
    "plot_curve('Linear Regression', 'MSE', train_sizes, \n",
    "             train_errors_mean, validation_errors_mean, ylim=(0,40))\n",
    "    \n",
    "    \n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnQxQMGewktI"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HHzP0wJ9wktP"
   },
   "source": [
    "The validation MSE seems to stagnate at a value of approximately 20. Is this good enough? \n",
    "\n",
    "We’d benefit from some domain knowledge.\n",
    "Technically, that value of 20 has MW (megawatts squared) as units (the units get squared as well when we compute the MSE). The values in our target column are in MW (according to the documentation). Taking the square root of 20 MW results in approximately 4.5 MW. Each target value represents net hourly electrical energy output. So for each hour our model is off by 4.5 MW on average. According to this [Quora](https://www.quora.com/How-can-I-get-an-intuitive-understanding-of-what-a-Kw-Mw-Gw-of-electricity-equates-to-in-real-life-terms) answer, 4.5 MW is equivalent to the heat power produced by 4500 handheld hair dryers. And this would add up if we tried to predict the total energy output for one day or a longer period. We can conclude that the an MSE of 20 MW is quite large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VhyttoD0wktR"
   },
   "source": [
    "***Exercise***:\n",
    "\n",
    "Examine the learning curve you plot, answer the following questions (don't look at the answer first).\n",
    "\n",
    "1. Is this a high-bias problem or a low-bias problem?\n",
    "\n",
    "<details><summary>Click here for answer</summary><p>High Bias</p></details>\n",
    "\n",
    "2. Is it high variance or low variance?\n",
    "\n",
    "<details><summary>Click here for answer</summary><p>Low Variance</p></details>\n",
    "\n",
    "3. Will adding more training data help to improve the performance of the model?\n",
    "\n",
    "<details><summary>Click here for answer</summary><p>No</p></details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fBr189UmwktT"
   },
   "source": [
    "We can try to reduce the bias with the following methods:\n",
    "- use a more complex learning algorithm\n",
    "- add more features (not samples) or try generate polynomial features from existing features\n",
    "- reduce regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PJfS7RV7wktV"
   },
   "source": [
    "Let's try using RandomForestRegressor instead. You don't need to know the details of RandomForestRegressor, and we are just using it to see how it impacts the bias/variance. \n",
    "\n",
    "***Exercise:*** \n",
    "\n",
    "Complete the code below to plot the learning curve. The steps are similar to the above. \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "train_sizes, train_errors, validation_errors = learning_curve(\n",
    "                    estimator, X, y, train_sizes = train_sizes, cv=5,\n",
    "                    scoring = 'neg_mean_squared_error',\n",
    "                    shuffle=True, random_state=42)\n",
    "    \n",
    "train_errors_mean = -train_errors.mean(axis = 1)\n",
    "validation_errors_mean = -validation_errors.mean(axis = 1)\n",
    "    \n",
    "plot_curve('RandomForest Regressor', 'MSE', train_sizes, train_errors_mean, validation_errors_mean, ylim=(0,40))\n",
    "\n",
    "    \n",
    "    \n",
    "```\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wFAtfKvGwktY"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "estimator = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6izl3gzwktf"
   },
   "source": [
    "***Exercise:***\n",
    "\n",
    "1. Does the new learning curve show a low or high bias?\n",
    "\n",
    "<details><summary>Click here for answer</summary><p>Low Bias</p></details>\n",
    "\n",
    "2. Does the new learning curve show a low or high variance?\n",
    "\n",
    "<details><summary>Click here for answer</summary><p>High Variance</p></details>\n",
    "\n",
    "3. Will adding more training data help to improve the performance of the model?\n",
    "\n",
    "<details><summary>Click here for answer</summary><p>Yes, this may help</p></details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnmzZbn1wkth"
   },
   "source": [
    "## 2. Learning Curve for Classification Problem ##\n",
    "\n",
    "First, let's get the dataset you will work on. The following code will load a \"[digits](https://scikit-learn.org/stable/datasets/index.html#digits-dataset)\" dataset into variables `X` and `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPUBvGfdwktj"
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4bcl0scwktw"
   },
   "source": [
    "You have:\n",
    "    - a numpy-array X that contains your features (the pixel values)\n",
    "    - a numpy-array Y that contains your labels (digits 0 to 9).\n",
    "\n",
    "Lets first get a better sense of what our data is like. \n",
    "\n",
    "***Exercise:***\n",
    "\n",
    "How many training examples do you have? In addition, what is the `shape` of the variables `X` and `Y`? \n",
    "\n",
    "***Hint***: How do you get the shape of a numpy array? [(help)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html)\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "shape_X = X.shape\n",
    "shape_Y = y.shape\n",
    "m = shape_X[0]\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sU3KHutEwkt3"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### \n",
    "\n",
    "shape_X = None\n",
    "shape_Y = None\n",
    "m = None\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "### Uncomment the codes below to print out the values ###\n",
    "#print ('The shape of X is: ' + str(shape_X))\n",
    "#print ('The shape of Y is: ' + str(shape_Y))\n",
    "#print ('I have m = %d training examples!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ak9iPK6wkuC"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:40%\" align=\"left\">\n",
    "  <tr>\n",
    "    <td><b>shape of X</b></td>\n",
    "    <td>(1797, 64)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>shape of Y</b></td>\n",
    "    <td>(1797,)</td> \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td><b>m</b></td>\n",
    "    <td>1797</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h0yV6MmQwkuG"
   },
   "source": [
    "Let us visualize some digit in the data set.\n",
    "\n",
    "***Exercise:***\n",
    "\n",
    "The original image is a 8 x 8 grey scale image. However, the sample in ``X`` is a numpy array of 64 values. Add codes below to transform the ``X[3]`` into a 8 x 8 image for plotting.\n",
    "\n",
    "**Hint**: How do you reshape a numpy array? [(help)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html#numpy.reshape)\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "some_digit = X[3]\n",
    "label = y[3]\n",
    "some_digit_image = some_digit.reshape(8, 8)\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q6SguCQYwkuJ"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### \n",
    "\n",
    "some_digit = None\n",
    "label = None \n",
    "some_digit_image = None \n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(some_digit_image, cmap = plt.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print('Label = {}'.format(label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVMqk3sXwkuc"
   },
   "source": [
    "***Exercise:***\n",
    "\n",
    "Create a cross-validation splits with 50 iterations to get smoother mean test and train\n",
    "score curves, each time with 20% data randomly selected as a validation set.\n",
    "\n",
    "**Hint**: Use the [ShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html) in scikit-learn \n",
    "\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=42)\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wpfPpl8Iwkui"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### \n",
    "\n",
    "cv = None \n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GdWEUa96wkup"
   },
   "source": [
    "**learning_curve()** expects a param called **train_sizes**, which are numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within (0, 1]. Otherwise it is interpreted as absolute sizes of the training sets. Note that for classification the number of samples usually have to be big enough to contain at least one sample from each class.\n",
    "\n",
    "***Exercise:***\n",
    "\n",
    "Divide the number training samples into 5 equal sizes, starting from 0.1 (i.e. 10% of the training samples). \n",
    "\n",
    "**Hint:**\n",
    "Use [numpy.linspace()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html)\n",
    "\n",
    "\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "train_sizes = np.linspace(0.1, 1.0, 5)\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iuyW8oSEwkuu"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### \n",
    "\n",
    "train_sizes = None \n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(train_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FEjzuHNswkvA"
   },
   "source": [
    "***Exercise:***\n",
    "\n",
    "Create a LogisticRegression estimator with solver='liblinear' and multi-class='auto' and call the ``learnin_curve()`` function to get the train and validation scores. You can set the ``cv`` param to the cross-validation you created earlier. You need to choose scoring metrics appropriate for classification problem (e.g. 'accuracy')\n",
    "\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "estimator = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "train_sizes, train_scores, validation_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, scoring='accuracy', train_sizes=train_sizes, shuffle=True)\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1_cm6-lwkvE"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### \n",
    "\n",
    "estimator = None \n",
    "train_sizes, train_scores, validation_scores = None \n",
    "\n",
    "### END CODE HERE ### \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AdeyyZB4wkvL"
   },
   "source": [
    "***Exercise:***\n",
    "\n",
    "What do you think are the shapes of the train_scores and test_scores?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "<p>\n",
    "Since we specify 5 training sizes, for each training, we specify a 50-fold cross-validations, we should have 5 x 50 train_scores and test_scores.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zt_UPMTCwkvM"
   },
   "outputs": [],
   "source": [
    "### Uncomment the following to check your answers\n",
    "\n",
    "#print(train_scores.shape)\n",
    "#print(validation_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJjVul36wkvX"
   },
   "source": [
    "***Exercise:*** \n",
    "\n",
    "To plot the learning curves, we need only a single score per training set size, not 50. To do this we need to take the mean value of 50 scores of each training/validation round. As the scores is the accuracy scores, you will need to convert them to error rate. \n",
    "\n",
    "***Hint:***  Fraction of error = 1.0 - (fraction of correct)\n",
    "\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "train_errors_mean = 1. - np.mean(train_scores, axis=1)\n",
    "validation_errors_mean = 1. - np.mean(validation_scores, axis=1)\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ul4NCkzswkvZ"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (~ 2 lines of code)\n",
    "\n",
    "train_errors_mean = None \n",
    "validation_errors_mean = None \n",
    "\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qa-JltaUwkvf"
   },
   "source": [
    "### Plot the learning curve ###\n",
    "Ok, now we can start plotting the curve. You should expect to see a learning curve similar to the following:\n",
    "\n",
    "<img src=\"https://github.com/nyp-sit/sdaai-iti103/blob/master/session-5/images/classification_lc.png?raw=1\" alt=\"classification learning curve\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agNlVEUqwkvh"
   },
   "source": [
    "***Exercise:***\n",
    "\n",
    "Plot the learning curve for logistic regression. \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "plot_curve('Logistic Regression', 'Error', train_sizes, train_errors_mean, validation_errors_mean)\n",
    "\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QsrbIDbywkvj"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### \n",
    "\n",
    "### END CODE HERE ### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k6v5hmqWwkvv"
   },
   "source": [
    "***Exercise:***\n",
    "\n",
    "Is this a high-bias or high variance problem?\n",
    "<details><summary>Click here for answer</summary><p>High variance</p></details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ao4UoETywkvx"
   },
   "source": [
    "Let us try a more complex non-linear algorithm such as Support Vector Machine (SVM). \n",
    "You don't need to know the details of SVM, and we are just using it to see how it impacts the bias/variance. \n",
    "\n",
    "As SVC takes longer to train, we reduce the split to 10 to speed-up the training time. \n",
    "\n",
    "***Exercise:***\n",
    "\n",
    "Complete the code below to plot the learning curve for SVM. \n",
    "\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "train_sizes, train_scores, validation_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, scoring='accuracy', train_sizes=train_sizes, shuffle=True)\n",
    "train_errors_mean = 1. - np.mean(train_scores, axis=1)\n",
    "validation_errors_mean = 1. - np.mean(validation_scores, axis=1)\n",
    "plot_curve('SVM', 'Error', train_sizes, train_errors_mean, validation_errors_mean)\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_6-R7aQwkv2"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = SVC(gamma=0.001)\n",
    "\n",
    "### START CODE HERE ### (~ 4 lines of code)\n",
    "\n",
    "\n",
    "### END CODE HERE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5ZF7SmGwkv9"
   },
   "source": [
    "***Exercise:***\n",
    "\n",
    "How does the use of SVM affect the bias of the model?\n",
    "\n",
    "<details><summary>Click here for answer</summary><p>Using a more complex, non-linear algoritm such as SVM improves the bias of the model</p></details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "plot_learning_curve.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.8 (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
