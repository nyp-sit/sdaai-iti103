{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Ex1_Regularization.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti103/blob/master/session-6/Ex1_Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLwc58I8N7Iu"
      },
      "source": [
        "# Regularization\n",
        "\n",
        "Welcome to the hands-on lab. This is part of the series of exercises to help you acquire skills in different techniques to fine-tune your model.\n",
        "\n",
        "In this lab, you will learn:\n",
        "- how regularization can be used to avoid overfitting the model\n",
        "- effects of different regularization techniques (e.g. L1/L2) on coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5olw6XoSN7Iw"
      },
      "source": [
        "## Part 1: Understanding Regularization Effects\n",
        "\n",
        "We will begin with a short tutorial on regularization based on a very simple generated 'noisy' dataset and examine the effects of regularization on the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHTh1nNvN7Ix"
      },
      "source": [
        "### 1.1 Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnSx44U5N7Ix"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', module='sklearn')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# To plot pretty figures\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anSrDGv8N7Iy"
      },
      "source": [
        "### 1.2. Import a Data Set and Plot the Data\n",
        "\n",
        "Import the file 'X_Y_Sinusoid_Data.csv' which contains a noisy set of x and y values that corresponds to a function $y = sin(2\\pi x)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuSgNpCkN7Iy"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/nyp-sit/sdaai-iti103/master/session-6/data/X_Y_Sinusoid_Data.csv'\n",
        "data = pd.read_csv(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz-ciob3N7Iy"
      },
      "source": [
        "Now we will create a set of x and y values that corresponds to the ground truth $y = sin(2\\pi x)$ and plot the sparse data (`x` vs `y`) and the calculated (\"real\") data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYHzKyfwN7Iy"
      },
      "source": [
        "X_real = np.linspace(0, 1.0, 100)\n",
        "Y_real = np.sin(2 * np.pi * X_real)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31bpEnG8N7Iz"
      },
      "source": [
        "#plot the 'noisy' data\n",
        "\n",
        "sns.set_style('white')\n",
        "sns.set_context('notebook')\n",
        "sns.set_palette('dark')\n",
        "\n",
        "plt.plot(data['x'], data['y'], \"o\", label='data')\n",
        "plt.xlabel(\"x data\", fontsize=18)\n",
        "plt.ylabel(\"y data\", fontsize=18, rotation='vertical')\n",
        "\n",
        "#plot the real function\n",
        "plt.plot(X_real, Y_real, ls='--', label='real function')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzqzBsGmN7Iz"
      },
      "source": [
        "### 1.3. Fit the model with higher-oder polynomial features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU5xafPfN7Iz"
      },
      "source": [
        "***Exercise:***\n",
        "\n",
        "Using the [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class from Scikit-learn's preprocessing library, create 20th order polynomial features based on the `data` dataframe read from csv.\n",
        "\n",
        "\n",
        "<details>\n",
        "    <summary>Click here for answer</summary>\n",
        "<p>\n",
        "\n",
        "```python\n",
        "X_data = data[['x']]\n",
        "Y_data = data['y']\n",
        "\n",
        "degree = 20\n",
        "pf = PolynomialFeatures(degree)\n",
        "X_poly = pf.fit_transform(X_data)\n",
        "\n",
        "```\n",
        "\n",
        "</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtT_Vaj6N7I0"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "### START CODE HERE ###\n",
        "\n",
        "# Extract the X- and Y- data from the dataframe \n",
        "X_data = None\n",
        "Y_data = None\n",
        "\n",
        "# Setup the polynomial features\n",
        "degree = None\n",
        "pf = None \n",
        "\n",
        "# Create the polynomial features\n",
        "X_poly = None\n",
        "\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "#print(X_poly.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DCvLaqxN7I0"
      },
      "source": [
        "X_data has the 20 data points. What do you think is the shape of X_poly? Print the X_poly to confirm.\n",
        "\n",
        "\n",
        "<details>\n",
        "    <summary>Click here for answer</summary>\n",
        "<p>\n",
        "\n",
        "(20,21). Although we specify degree 20 for the polynomial features, 21 features were generated because of the additional bias term. You can omit the bias term by specifying:\n",
        "    \n",
        "```python\n",
        "pf = PolynomialFeatures(degree, include_bias=False)\n",
        "```\n",
        "\n",
        "</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_rIPCDgN7I0"
      },
      "source": [
        "Now we fit this data using linear regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUE3Vx8qN7I0"
      },
      "source": [
        "lr = LinearRegression()\n",
        "\n",
        "lr = lr.fit(X_poly, Y_data)\n",
        "Y_pred = lr.predict(X_poly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLF5Grm4N7I1"
      },
      "source": [
        "Plot the resulting predicted value compared to the calculated data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0wPNCmtN7I1"
      },
      "source": [
        "# Plot the result\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(X_data, Y_data, marker='o', ls='', label='data', alpha=1.0)\n",
        "plt.plot(X_real, Y_real, ls='--', label='real function')\n",
        "plt.plot(X_data, Y_pred, marker='^', alpha=0.5, label='pred w/ polynomial features')\n",
        "plt.legend()\n",
        "plt.xlabel('x data')\n",
        "plt.ylabel('y data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BByYlgzAN7I1"
      },
      "source": [
        "***Exercise:***\n",
        "\n",
        "What can you observe from the graph about the linear regression model trained with 20th degree polynomial features?\n",
        "<br>\n",
        "<details><summary>Click here for answer</summary>\n",
        "The model overfits the data.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUdTRLlHN7I1"
      },
      "source": [
        "### 1.4. Use Regularized Model\n",
        "\n",
        "Now we will use the regularized model such as Ridge and Lasso to fit the data with 20th degree polynomial features and observe the difference. \n",
        "\n",
        "***Exercise:***\n",
        "\n",
        "- Perform the regression on the data with polynomial features using ridge regression ($alpha$=0.001) and lasso regression ($alpha$=0.0001). \n",
        "- Plot the results, as was done in section 1.3. \n",
        "\n",
        "<details>\n",
        "    <summary>Click here for answer</summary>\n",
        "<p>\n",
        "    \n",
        "```python\n",
        "# Fit with ridge regression model\n",
        "ridge = Ridge(alpha=0.0001)\n",
        "ridge = ridge.fit(X_poly, Y_data)\n",
        "Y_pred_ridge = ridge.predict(X_poly)\n",
        "    \n",
        "# Fit with lasso regression model\n",
        "lasso = Lasso(alpha=0.0001)\n",
        "lasso = lasso.fit(X_poly, Y_data)\n",
        "Y_pred_lasso = lasso.predict(X_poly)\n",
        "``` \n",
        "</p>\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brLDP2EaN7I2"
      },
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "### START CODE HERE ###\n",
        "\n",
        "# Fit with ridge regression model\n",
        "\n",
        "\n",
        "\n",
        "# Similarly, fit the data with lasso regression model\n",
        "\n",
        "\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(X_data, Y_data, marker='o', ls='', label='data')\n",
        "plt.plot(X_real, Y_real, ls='--', label='real function')\n",
        "plt.plot(X_data, Y_pred, label='linear regression', marker='^', alpha=.5)\n",
        "plt.plot(X_data, Y_pred_ridge, label='ridge regression', marker='^', alpha=.5)\n",
        "plt.plot(X_data, Y_pred_lasso, label='lasso regression', marker='^', alpha=.5)\n",
        "\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xMowFmRN7I2"
      },
      "source": [
        "# let's look at the absolute value of coefficients for each model\n",
        "\n",
        "coefficients = pd.DataFrame()\n",
        "coefficients['linear regression'] = lr.coef_\n",
        "coefficients['ridge regression'] = ridge.coef_\n",
        "coefficients['lasso regression'] = lasso.coef_\n",
        "coefficients = coefficients.applymap(abs)\n",
        "\n",
        "coefficients.describe()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLFWGAZPN7I2"
      },
      "source": [
        "***Exercise***\n",
        "\n",
        "What do you observe about the differences among coefficients of linear regression, ridge regression and lasso regression? \n",
        "<br>\n",
        "<details>\n",
        "    <summary>Click here for answer</summary>\n",
        "\n",
        "The coefficients of non-regularized linear regression are very large, whereas the ridge regression and lasso regression have smaller coefficients.  For lasso too, there are also many coefficients that are 0 (25th percentile have values that are 0). \n",
        "\n",
        "Regularization (L1 or L2) shrinks the sizes of coefficients.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZvEMK3tN7I3"
      },
      "source": [
        "Here we plot the magnitude of the coefficients obtained from these regressions, and compare them to those obtained from linear regression in the previous question. The linear regression coefficients is plot using its own y-axis due to their much larger magnitude. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7E3vv91N7I3"
      },
      "source": [
        "colors = sns.color_palette()\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "# Setup the dual y-axes\n",
        "ax1 = plt.axes()\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "# Plot the linear regression data\n",
        "ax1.plot(lr.coef_, \n",
        "         color=colors[0], marker='o', label='linear regression')\n",
        "\n",
        "# Plot the regularization data sets\n",
        "ax2.plot(ridge.coef_, \n",
        "         color=colors[1], marker='o', label='ridge regression')\n",
        "\n",
        "ax2.plot(lasso.coef_, \n",
        "         color=colors[2], marker='o', label='lasso regression')\n",
        "\n",
        "# Customize axes scales\n",
        "ax1.set_ylim(-2e14, 2e14)\n",
        "ax2.set_ylim(-25, 25)\n",
        "\n",
        "# Combine the legends\n",
        "h1, l1 = ax1.get_legend_handles_labels()\n",
        "h2, l2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(h1+h2, l1+l2)\n",
        "\n",
        "ax1.set(xlabel='coefficients',ylabel='linear regression')\n",
        "ax2.set(ylabel='ridge and lasso regression')\n",
        "\n",
        "ax1.set_xticks(range(len(lr.coef_)));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w2icquWN7I3"
      },
      "source": [
        "## Part 2 - Applying Regularization on Boston housing prices dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIoA71pPN7I3"
      },
      "source": [
        "Now that we have seen the effects of L1/L2 regularization on the coefficients of linear model, we will now apply them on real dataset, the Boston housing prices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggTsRk1MN7I4"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
        "\n",
        "def load_scaled_boston_data():\n",
        "    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "    target = raw_df.values[1::2, 2]\n",
        "    \n",
        "    ## Never do this in real ML project. Here we fit the scalar to entire dataset, however,\n",
        "    ## our purpose here is just have a scaled dataset to use\n",
        "    X = MinMaxScaler().fit_transform(data)\n",
        "    X = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X)\n",
        "    return X, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIOId4HAN7I4"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "import numpy as np\n",
        "\n",
        "X, y = load_scaled_boston_data()\n",
        "\n",
        "# Split the data into train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "print(\"Number of features in the datase: {}\".format(X.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5VAO9UuN7I4"
      },
      "source": [
        "Here we compute the $R^2$ score (for easier comparison) of linear regression for both train and test score too see if there is overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XygwngFoN7I4"
      },
      "source": [
        "lr = LinearRegression().fit(X_train, y_train)\n",
        "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n",
        "print(\"Number of features used: {}\".format(np.sum(lr.coef_ != 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKfoVI_CN7I4"
      },
      "source": [
        "Let's compare the scores with L2-regularized version using Ridge Regression, using different regularization strength (alpha)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTokRMK6N7I5"
      },
      "source": [
        "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
        "print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))\n",
        "print(\"Number of features used: {}\".format(np.sum(lr.coef_ != 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIV3o3ZYN7I5"
      },
      "source": [
        "ridge1 = Ridge(alpha=1).fit(X_train, y_train)\n",
        "print(\"Training set score: {:.2f}\".format(ridge1.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(ridge1.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDjmINbGN7I5"
      },
      "source": [
        "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
        "print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))\n",
        "print(\"Number of features used: {}\".format(np.sum(lr.coef_ != 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACRN2DhjN7I5"
      },
      "source": [
        "***Exercises:***\n",
        "\n",
        "How does regularization strength affects the bias and variance of the model? \n",
        "<br/>\n",
        "<details><summary>Click here for answer</summary>\n",
        "<p> \n",
        "In general, by increasing the regularization (increase values of alpha), we can see that the bias increases while the variance decreases.  \n",
        "</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfoW4T_6N7I5"
      },
      "source": [
        "Let's inspect visually the coefficients of both linear regression and ridge regression with different regularization strength."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8ckNf_bN7I6"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(ridge1.coef_, 's', label=\"Ridge alpha=1\")\n",
        "plt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\n",
        "plt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\n",
        "plt.plot(lr.coef_, 'o', label=\"LinearRegression\")\n",
        "plt.xlabel(\"Coefficient index\")\n",
        "plt.ylabel(\"Coefficient magnitude\")\n",
        "plt.hlines(0, 0, len(lr.coef_))\n",
        "plt.ylim(-25, 25)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Udzob2oN7I6"
      },
      "source": [
        "As before, we can see that coefficients for non-regularized linear regression are much larger than regularized version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HF8rAhKN7I6"
      },
      "source": [
        "Now let's compare Linear Regression with Lasso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NboaUK8N7I6"
      },
      "source": [
        "***Exercises:***\n",
        "\n",
        "- Now fit the data using Lasso, with different alpha values \\[0.0001, 0.01, 1.0 (default), 10\\]\n",
        "- For each lasso model, print the training set score, test set score and also number of coefficients not zero\n",
        "\n",
        "<br/>\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "lasso00001 = Lasso(alpha=0.0001).fit(X_train, y_train)\n",
        "print(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\n",
        "print(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))\n",
        "\n",
        "lasso001 = Lasso(alpha=0.01).fit(X_train, y_train)\n",
        "print(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\n",
        "print(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))\n",
        "    \n",
        "lasso1 = Lasso(alpha=1.0).fit(X_train, y_train)\n",
        "print(\"Training set score: {:.2f}\".format(lasso1.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(lasso1.score(X_test, y_test)))\n",
        "print(\"Number of features used: {}\".format(np.sum(lasso1.coef_ != 0)))\n",
        "\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x8h62qvN7I6"
      },
      "source": [
        "Now fit the data using Lasso, with different alpha values \\[0.0001, 0.01, 1.0 (default)\\]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFXvtq_3N7I7"
      },
      "source": [
        "### START CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCZjgNBuN7I7"
      },
      "source": [
        "***Exercise***\n",
        "\n",
        "What happen to bias and variance when you increase the alpha of Lasso regression? Why is that so?\n",
        "<br>\n",
        "<details>\n",
        "    <summary>Click here for answer</summary>\n",
        "<p>\n",
        "The bias has gone up and the variance has comed down. The last model with alpha=1 has such a high bias because Lasso drives most of the coefficients to 0 and left with 3 features. This is too simple a model and thus is not able to fit the training data well, thus the high bias.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsiu-ISTN7I7"
      },
      "source": [
        "Let us display the absolute values (ignore the negative sign) of the coefficients in a dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aia30uwQN7I7"
      },
      "source": [
        "coefficients = pd.DataFrame()\n",
        "coefficients['linear regression'] = lr.coef_ \n",
        "coefficients['lasso alpha=0.0001'] = lasso00001.coef_ \n",
        "coefficients['lasso alpha=0.001'] = lasso001.coef_ \n",
        "coefficients['lasso alpha=1.0'] = lasso1.coef_\n",
        "coefficients = coefficients.applymap(abs)\n",
        "coefficients.describe()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPk_j3f2N7I7"
      },
      "source": [
        "Let's just visually inspect the  values of coefficients for lasso regression with different regularization strength. We can see that at alpha=1, most coefficients are zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLqqKPaxN7I7"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(lr.coef_, 'o', label=\"linear regression\")\n",
        "plt.plot(lasso00001.coef_, 's', label=\"Lasso alpha=0.0001\")\n",
        "plt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\n",
        "plt.plot(lasso1.coef_, 'v', label=\"Lasso alpha=1\")\n",
        "plt.legend(ncol=2, loc=(0, 1.05))\n",
        "plt.ylim(-20, 20)\n",
        "plt.xlabel(\"Coefficient index\")\n",
        "plt.ylabel(\"Coefficient magnitude\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk_vTuW6Se23"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}