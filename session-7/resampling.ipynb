{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oversampling(Solution).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnqURdKXOvF9"
      },
      "source": [
        "# Dealing with Imbalanced Data Set\n",
        "\n",
        "Welcome to the hands-on lab. This is part of the series of exercises to help you acquire skills in different techniques to fine-tune your model.\n",
        "\n",
        "In this lab, you will learn:\n",
        "- how to use resampling correctly for imbalanced data set\n",
        "- how to perform resampling using K-folds\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RwnjO0pOvF-"
      },
      "source": [
        "In this exercise, we will use a highly imbalanced data set from Lending Club that consists of data for both 'bad' and 'good' loans to illustrate how we can apply oversampling and undersampling techniques to improve our model performance. You will also learn to apply resampling correctly when using cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLhPfND5OvF-"
      },
      "source": [
        "## Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1155FtcFOvF_"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', module='sklearn')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score, recall_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4uzbQmnOvGE"
      },
      "source": [
        "## Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKm5pvtTOvGF"
      },
      "source": [
        "url = 'https://github.com/nyp-sit/data/raw/master/lending-club-data.csv.zip'\n",
        "zip_file = \"lending_club-data.csv.zip\"\n",
        "\n",
        "# download the zip file and copy to a file 'lending-club-data.csv.zip'\n",
        "with urllib.request.urlopen(url) as response, open(zip_file, 'wb') as out_file:\n",
        "    shutil.copyfileobj(response, out_file)\n",
        "    \n",
        "# unzip the file to a folder 'data'\n",
        "data_file = 'lending_club_data.csv'\n",
        "\n",
        "with zipfile.ZipFile(zip_file,\"r\") as zip_ref:\n",
        "    zip_ref.extractall('data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUjfEldbOvGI"
      },
      "source": [
        "## Exploratory Data analysis\n",
        "\n",
        "Here we are trying to find out some information about the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KUTDi-oOvGJ"
      },
      "source": [
        "df = pd.read_csv('data/lending-club-data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1PMtr7wOvGP"
      },
      "source": [
        "Let us just find out about different features and their data types. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KphBoylOvGQ"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGorNXkCOvGU"
      },
      "source": [
        "In this exercise, we are trying to predict if a member will default on his loan or not. So we will be using the feature column 'bad_loans' as the label for our classification task. If the value of `bad_loan` is 1, it means it is a default (or bad loan), otherwise, it is 0.  \n",
        "\n",
        "***Exercise:***\n",
        "\n",
        "Find out how many samples in the data set is bad loans and how many are not. \n",
        "\n",
        "Hint: `value_counts()` in [pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html) give you the count of unique values\n",
        "\n",
        "<p>\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "\n",
        "df.bad_loans.value_counts()\n",
        "\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxAwMqOLOvGV"
      },
      "source": [
        "## Complete the code below ## \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbTZ3HYROvGZ"
      },
      "source": [
        "Is the data set imbalanced? Clearly we have a lot of more good loans than bad loans (around 4 times more)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-699YfGOvGZ"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqFZxW3POvGa"
      },
      "source": [
        "There are quite a lot of features in this data set but we are just going to use a few, just for demonstration purpose (as we are not really interested in actual performance of our model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiB1_iyKOvGb"
      },
      "source": [
        "features = ['grade', 'home_ownership','emp_length_num', 'sub_grade','short_emp',\n",
        "            'dti', 'term', 'purpose', 'int_rate', 'last_delinq_none', 'last_major_derog_none',\n",
        "            'revol_util', 'total_rec_late_fee', 'payment_inc_ratio', 'bad_loans']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThOEfOuTOvGe"
      },
      "source": [
        "***Exercise:*** \n",
        "\n",
        "Create a data frame that consist of the subset of features listed above.\n",
        "\n",
        "<p>\n",
        "<details><summary>Click here for answer</summary>\n",
        "    \n",
        "```python\n",
        "df = df[features]\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcD4E6QMOvGf"
      },
      "source": [
        "## Complete the code below ## \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I0zJTN0OvGi"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54vsiwgBOvGm"
      },
      "source": [
        "Notice that `payment_inc_ratio` has some null values, and since it is only a small number, just remove the rows that have null values for `payment_inc_ratio`.\n",
        "\n",
        "***Exercise***\n",
        "\n",
        "Create a new data frame that have the rows that contains null values for `payment_inc_ratio` removed. \n",
        "\n",
        "<p>\n",
        "<details><summary>Click here for answer</summary>\n",
        "    \n",
        "```python\n",
        "loans_df = df.dropna()\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2LnIJIEOvGn"
      },
      "source": [
        "## Complete the code below ## \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kvTJH5-OvGq"
      },
      "source": [
        "loans_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdYkuMW2OvGs"
      },
      "source": [
        "***Exercise:*** \n",
        "\n",
        "Encode the categorical columns (dtype=object). You can use the convenience method `get_dummies()` provide by [pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html).\n",
        "\n",
        "<p>\n",
        "<details><summary>Click here for answer</summary>\n",
        "    \n",
        "```python\n",
        "loans_encoded = pd.get_dummies(loans_df)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtXWlMJuOvGt"
      },
      "source": [
        "## Complete the code below ## \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwRvHWKUr99i"
      },
      "source": [
        "loans_encoded.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pplOyf8XOvGz"
      },
      "source": [
        "### Split the data set into train and test set\n",
        "\n",
        "***Exercise:*** \n",
        "\n",
        "First, separate the features and the label.  \n",
        "\n",
        "Hint: use `df.drop()` and specify `axis=1` to remove a particular column in dataframe.\n",
        "\n",
        "Then, split the data into train set (called `X_train, y_train`) and test set (`X_test, y_test`). Think about the splitting strategy, e.g. do you need to ensure the distribution of good/bad is the same in both train and test set?\n",
        "\n",
        "<p>\n",
        "<details><summary>Click here for answer</summary>\n",
        "    \n",
        "```python\n",
        "\n",
        "X_df = loans_encoded.drop(['bad_loans'], axis=1)\n",
        "y_df = loans_encoded['bad_loans']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, \n",
        "                                                    test_size = .2, \n",
        "                                                    stratify = y_df,\n",
        "                                                    random_state = 42)\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV7UrEgrOvG0"
      },
      "source": [
        "## Complete the code below ## \n",
        "\n",
        "# X_df contains all the feature columns and y_df contains only the label, i.e. bad_loans column\n",
        "\n",
        "X_df = None\n",
        "y_df = None\n",
        "\n",
        "# split the data into train and test set\n",
        "X_train, X_test, y_train, y_test = None\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeEr2-jBOvG4"
      },
      "source": [
        "print(y_train.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raOmwSbHrf-X"
      },
      "source": [
        "## Train a baseline model\n",
        "\n",
        "Now for comparison sake, we will train a classifier and see its performance on the test set.\n",
        "As we are interested in knowing how well our model is in picking out 'bad loan', it would be useful to look at the recall score of the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXfRvO7Prf-X"
      },
      "source": [
        "clf_rf = RandomForestClassifier(n_estimators=25, random_state=42)\n",
        "clf_rf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxiSeksvrf-X"
      },
      "source": [
        "y_test_pred = clf_rf.predict(X_test)\n",
        "\n",
        "print(precision_score(y_test, y_test_pred))\n",
        "print(recall_score(y_test, y_test_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGhqO4Jerf-Y"
      },
      "source": [
        "## Oversampling\n",
        "\n",
        "Now we will try the over-sampling techniques to see if we can improve our model performance on the 'bad loan'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms9xSxclOvG-"
      },
      "source": [
        "### The ***wrong*** way to oversample ###\n",
        "\n",
        "With the training data created, we can oversample the minority class (the bad_loan = 1). In this exercise, we will use the SMOTE (from the [imblearn](https://imbalanced-learn.readthedocs.io/en/stable/index.html) library) to create synthetic samples of the minority class. \n",
        "\n",
        "After upsampling to a class ratio of 1.0 (i.e. 1 to 1 ratio between positive and negative classes) you should have a balanced dataset. In most cases, there’s often no need to balance the classes totally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09afPcnaOvG_"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Set sampling_strategy='minority' to oversample only the minority class \n",
        "\n",
        "sm = SMOTE(sampling_strategy='minority',random_state=42)\n",
        "X_upsample, y_upsample = sm.fit_resample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYewJVUYOvHB"
      },
      "source": [
        "Now let's see the number of samples we have for each class. You will see that now our train set is totally balanced, with equal number of samples for each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzcmiL-YOvHC"
      },
      "source": [
        "y_upsample.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxd1sYiJOvHG"
      },
      "source": [
        "Now let us split the up-sampled training data set into training and validation set.\n",
        "\n",
        "***Note:***\n",
        "\n",
        "It might be a bit confusing as we talk about training sets. We have our original data set, `X` and we split into `X_train` and `X_test`.  We up-sample the `X_train` to get `X_upsample`. And then from the `X_upsample`, we further set aside a train set and validation set, which we call: `X_train_final`, and `X_val_final` to differentiate from the earlier `X_train` and `X_upsample`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8ewnQ5JOvHI"
      },
      "source": [
        "#now split into train/validation sets\n",
        "\n",
        "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(X_upsample, y_upsample, \n",
        "                                                                          test_size=.1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyl14DBIOvHL"
      },
      "source": [
        "We then train a classifier and look at the performance of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5LXxQ5IOvHM"
      },
      "source": [
        "clf_rf = RandomForestClassifier(n_estimators=25, random_state=42)\n",
        "clf_rf.fit(X_train_final, y_train_final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANosUgZNOvHS"
      },
      "source": [
        "Let's see how well our model performs on the validation set, i.e. `X_val_final`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYGhDwZDOvHS"
      },
      "source": [
        "y_val_final_pred = clf_rf.predict(X_val_final)\n",
        "\n",
        "print(precision_score(y_val_final, y_val_final_pred))\n",
        "print(recall_score(y_val_final, y_val_final_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdknGvRgOvHV"
      },
      "source": [
        "Our precision and recall has improved to 93% and and 82% respectively. Impressive!  But is this actually representative of how the model will perform? Let's put our model to test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au3pbo7OOvHV"
      },
      "source": [
        "y_test_pred = clf_rf.predict(X_test)\n",
        "\n",
        "print(precision_score(y_test, y_test_pred))\n",
        "print(recall_score(y_test, y_test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INAtz44QOvHa"
      },
      "source": [
        "That’s disappointing! What has happened?\n",
        "\n",
        "By oversampling before splitting into training and validation datasets, we “leaked” information from the validation set into the training of the model (refer to your lecture for more details)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-fkAmtpOvHb"
      },
      "source": [
        "### The ***right way*** to oversample\n",
        "\n",
        "So, let do it the right way and see what happens. This time round, we will oversample the training set and not the train + validation set. Oversampling is done after we set aside the validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcdqazCuOvHc"
      },
      "source": [
        "## Here we set aside a validation set first \n",
        "\n",
        "X_train_proper,  X_val_proper, y_train_proper, y_val_proper = train_test_split(X_train, y_train, test_size=.1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da-q1LuFOvHe"
      },
      "source": [
        "Now as before, we use SMOTE to oversample the minority class, but this time we only oversample from the train set.  \n",
        "\n",
        "***Exercise:***\n",
        "\n",
        "Use SMOTE (as before) to over-sample the `X_train_proper`. Train a classifier as before and evaluate the classifier on validation data (X_val_proper). \n",
        "\n",
        "What is your precision and recall score now?\n",
        "\n",
        "<p>\n",
        "<details><summary>Click here for answer</summary>\n",
        "    \n",
        "```python\n",
        "\n",
        "sm = SMOTE(sampling_strategy='minority',random_state=42) \n",
        "X_train_proper_upsampled, y_train_proper_upsampled = sm.fit_sample(X_train_proper, y_train_proper)\n",
        "    \n",
        "clf_rf = RandomForestClassifier(n_estimators=25, random_state=42)\n",
        "clf_rf.fit(X_train_proper_upsampled, y_train_proper_upsampled)\n",
        "\n",
        "y_val_proper_pred = clf_rf.predict(X_val_proper)\n",
        "\n",
        "print(precision_score(y_val_proper, y_val_proper_pred))\n",
        "print(recall_score(y_val_proper, y_val_proper_pred))\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM6n8hrKOvHf"
      },
      "source": [
        "## Complete the code below ## \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBN7ujwFrf-b"
      },
      "source": [
        "Let's see if this validation result is closer to what we will get for the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5zbMK5KOvHq"
      },
      "source": [
        "y_test_pred = clf_rf.predict(X_test)\n",
        "\n",
        "print(precision_score(y_test, y_test_pred))\n",
        "print(recall_score(y_test, y_test_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYkuhnyIOvHu"
      },
      "source": [
        "Now, we can see that the recall rate obtained from the validation set matches more closely the result from the test set, which is about 18% recall. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZG0w03bOvHw"
      },
      "source": [
        "### Oversampling when doing K-Fold \n",
        "\n",
        "If you are doing K-fold cross validation, you can use the pipeline to help you do that. However, sklearn pipeline expects each transformer in the pipeline to implement TransformerMixin or BaseEstimator interfaces. However, imblearn classes like SMOTE does not. Fortunately imblearn provides its own pipeline implementation, which we can use to replace the sklearn pipeline. In the code below, we use the imblearn Pipeline to do first oversample our minority class in train folds, train a classifier on train folds, and validate on the validation fold. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDUVpGdDrf-c"
      },
      "source": [
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=25, random_state=42)\n",
        "\n",
        "# declare a pipeline that consists of the oversampler and the classifier\n",
        "steps = [('ovr', sm), ('clf', clf)]\n",
        "pipeline = Pipeline(steps=steps)\n",
        "scoring = ['precision','recall']\n",
        "\n",
        "# the oversampling is only applied to the train folds\n",
        "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
        "scores = cross_validate(pipeline, X_train, y_train, scoring=scoring, cv=cv, n_jobs=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eFDFE4orf-c"
      },
      "source": [
        "## Undersampling\n",
        "\n",
        "It does not seems that we have much success with oversampling. Let us try undersampling to see if we can get a better model.\n",
        "\n",
        "**Exercise:**\n",
        "\n",
        "Complete the code cell below, using RandomUndersampler, resample only the majority class. Cross-validate with RandomForestClassifier like before and compare the result with the oversampling approach. What do you observe about the result?\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "<br/>\n",
        "    \n",
        "```python\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline \n",
        "\n",
        "\n",
        "undersampler  = RandomUnderSampler(sampling_strategy=1.0, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=25, random_state=42)\n",
        "\n",
        "# declare a pipeline that consists of the oversampler and the classifier\n",
        "steps = [('under', undersampler), ('clf', clf)]\n",
        "pipeline = Pipeline(steps=steps)\n",
        "\n",
        "# let's monitor precision recall scores\n",
        "scoring = ['precision','recall']\n",
        "\n",
        "# the oversampling is only applied to the train folds\n",
        "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
        "scores = cross_validate(pipeline, X_train, y_train, scoring=scoring, cv=cv, n_jobs=-1)\n",
        "\n",
        "print('average precision: {}', scores['test_precision'].mean())\n",
        "print('average recall: {}', scores['test_recall'].mean())\n",
        "    \n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL6j0O6Prf-c"
      },
      "source": [
        "## Complete your code below ##\n",
        "\n",
        "# import the packages\n",
        "\n",
        "\n",
        "# create the RandomUndersampler and classifer \n",
        "\n",
        "\n",
        "# declare a pipeline that consists of the oversampler and the classifier\n",
        "\n",
        "\n",
        "# let's monitor precision recall scores\n",
        "\n",
        "\n",
        "# use cross validation to compute the scores \n",
        "\n",
        "\n",
        "# print the precision recall scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuV17vxOrf-d"
      },
      "source": [
        "### Combining Oversampling and Undersampling\n",
        "\n",
        "Can we do better by combining oversampling and undersampling? Let's find out. \n",
        "\n",
        "**Exercise:**\n",
        "\n",
        "Complete the codes below to first downsample the majority class such that minority is 50% of majority class, and then upsample the minority to same as majority. \n",
        "\n",
        "Compare the result with previous oversample-only and undersample-only results. What do you observe? \n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "<br/>\n",
        "We observe the the precision has improved compared to undersample-only but recall has gone down compared to undersample-only. \n",
        "<br/>\n",
        "\n",
        "```python\n",
        "# first downsample the majority classes such that minority class is 50% of majority class\n",
        "undersampler  = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
        "# we then upsample the minority class such that both are the same ratio\n",
        "oversampler = SMOTE(sampling_strategy=1.0, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=25, random_state=42)\n",
        "\n",
        "# declare a pipeline that first undersample, and then oversample, followed by classifier\n",
        "steps = [('under', undersampler), ('over', oversampler), ('clf', clf)]\n",
        "\n",
        "pipeline = Pipeline(steps=steps)\n",
        "\n",
        "# let's monitor precision recall scores\n",
        "scoring = ['precision','recall']\n",
        "\n",
        "# the oversampling is only applied to the train folds\n",
        "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
        "scores = cross_validate(pipeline, X_train, y_train, scoring=scoring, cv=cv, n_jobs=-1)\n",
        "    \n",
        "# print the results \n",
        "print('average precision: {}', scores['test_precision'].mean())\n",
        "print('average recall: {}', scores['test_recall'].mean())\n",
        "```\n",
        "</details>\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdmqxXT9rf-d"
      },
      "source": [
        "## Complete the code below ## \n",
        "\n",
        "# create a downsampler that downsamples the majority classes such that minority class is 50% of majority class\n",
        "\n",
        "\n",
        "# create a upsampler that upsample the minority class such that both are the same ratio\n",
        "\n",
        "\n",
        "# create the classifier \n",
        "\n",
        "\n",
        "# declare a pipeline that first undersample, and then oversample, followed by classifier\n",
        "\n",
        "\n",
        "# let's monitor precision recall scores\n",
        "\n",
        "\n",
        "# run the cross validation here and print the results\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}